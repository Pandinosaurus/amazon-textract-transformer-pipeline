{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Post-Processing Amazon Textract with Location-Aware Transformers**\n",
    "\n",
    "# Part 2: Data Consolidation and Model Training/Deployment\n",
    "\n",
    "> *This notebook works well with the `Python 3 (Data Science)` kernel on SageMaker Studio*\n",
    "\n",
    "In the [first notebook](1.%20Data%20Preparation.ipynb) we worked through preparing a corpus with Amazon Textract and labelling a small sample to highlight entities of interest.\n",
    "\n",
    "In this part 2, we'll consolidate the labelling results together with a pre-prepared augmentation set, and actually train and deploy a SageMaker model for word classification.\n",
    "\n",
    "First, as in the previous notebook, we'll start by importing the required libraries and loading configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python Built-Ins:\n",
    "from datetime import datetime\n",
    "import json\n",
    "from logging import getLogger\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3  # AWS SDK for Python\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace as HuggingFaceEstimator\n",
    "from tqdm.notebook import tqdm  # Progress bars\n",
    "\n",
    "# Local Dependencies:\n",
    "import util\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "# Manual configuration (check this matches notebook 1):\n",
    "bucket_name = sagemaker.Session().default_bucket()\n",
    "bucket_prefix = \"textract-transformers/\"\n",
    "print(f\"Working in bucket s3://{bucket_name}/{bucket_prefix}\")\n",
    "config = util.project.init(\"ocr-transformers-demo\")\n",
    "print(config)\n",
    "\n",
    "# Field configuration saved from first notebook:\n",
    "with open(\"data/field-config.json\", \"r\") as f:\n",
    "    fields = [\n",
    "        util.postproc.config.FieldConfiguration.from_dict(cfg)\n",
    "        for cfg in json.loads(f.read())\n",
    "    ]\n",
    "entity_classes = [f.name for f in fields]\n",
    "\n",
    "# S3 URIs as per first notebook:\n",
    "imgs_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/imgs-clean\"\n",
    "textract_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/textracted\"\n",
    "annotations_base_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/annotations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Consolidation\n",
    "\n",
    "To construct a training set, we'll typically need to consolidate the results of multiple SageMaker Ground Truth labelling jobs: Perhaps because the work was split up into more manageable chunks - or maybe because additional review/adjustment jobs were run to improve label quality.\n",
    "\n",
    "First, we'll download the output folders of all our labelling jobs to the local `data/annotations` folder: (The code here assumes you configured the same `annotations_base_s3uri` output folder for each job in SMGT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync --quiet $annotations_base_s3uri ./data/annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside this folder, you'll find some **pre-annotated augmentation data** provided for you already (in the `augmentation-` subfolders). These datasets are not especially large or externally useful, but will help you train a better model without too much (or even any!) manual annotation effort.\n",
    "\n",
    "▶️ **Edit** the `include_jobs` line below to control which datasets (pre-provided and your own) will be included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_jobs = [\n",
    "    \"augmentation-1\",\n",
    "    \"augmentation-2\",\n",
    "    # TODO: Adjust the below to match the labelling jobs you created, or comment out if you didn't:\n",
    "    \"cfpb-boxes-1\",\n",
    "]\n",
    "\n",
    "\n",
    "source_manifests = []\n",
    "for job_name in sorted(filter(\n",
    "    lambda n: os.path.isdir(f\"data/annotations/{n}\"),\n",
    "    os.listdir(\"data/annotations\")\n",
    ")):\n",
    "    if job_name not in include_jobs:\n",
    "        logger.warning(f\"Skipping {job_name} (not in include_jobs list)\")\n",
    "        continue\n",
    "    job_manifest_path = f\"data/annotations/{job_name}/manifests/output/output.manifest\"\n",
    "    if not os.path.isfile(job_manifest_path):\n",
    "        raise RuntimeError(f\"Could not find job output manifest {job_manifest_path}\")\n",
    "    source_manifests.append({ \"job_name\": job_name, \"manifest_path\": job_manifest_path })\n",
    "\n",
    "print(f\"Got {len(source_manifests)} annotated manifests:\")\n",
    "print(\"\\n\".join(map(lambda o: o[\"manifest_path\"], source_manifests)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the results are downloaded, we're ready to consolidate the **output manifest files** from each one into a combined manifest file.\n",
    "\n",
    "Note that to combine multiple output manifests to a single dataset:\n",
    "\n",
    "- We need to ensure the labels are stored in the same attribute on every record (records use the labeling job name by default, which will be different between jobs).\n",
    "- If importing data collected from some other account (like the `augmentation-` sets), we'll need to **map the S3 URIs** to equivalent links on your own bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotations/labels will be standardized to this field on all records:\n",
    "standard_label_field = \"label\"\n",
    "\n",
    "# To import a manifest from somebody else, we of course need to map their bucket names and prefixes\n",
    "# to ours (and have equivalent files stored in the same locations after the mapping):\n",
    "BUCKET_MAPPINGS = { \"DOC-EXAMPLE-BUCKET\": bucket_name }\n",
    "PREFIX_MAPPINGS = { \"EXAMPLE-PREFIX/\": bucket_prefix }\n",
    "\n",
    "annotated_page_imgs = {}\n",
    "print(\"Writing data/annotations/annotations-all.manifest.jsonl\")\n",
    "with open(\"data/annotations/annotations-all.manifest.jsonl\", \"w\") as fout:\n",
    "    for source in tqdm(source_manifests, desc=\"Consolidating manifests...\"):\n",
    "        with open(source[\"manifest_path\"], \"r\") as fin:\n",
    "            for line in filter(lambda l: l, fin):\n",
    "                obj = json.loads(line)\n",
    "\n",
    "                # Import refs by applying BUCKET_MAPPINGS and PREFIX_MAPPINGS:\n",
    "                for k in filter(lambda k: k.endswith(\"-ref\"), obj.keys()):\n",
    "                    if not obj[k].lower().startswith(\"s3://\"):\n",
    "                        raise RuntimeError(\n",
    "                            \"Attr {} ends with -ref but does not start with 's3://'\\n{}\".format(\n",
    "                                k,\n",
    "                                obj\n",
    "                            )\n",
    "                        )\n",
    "                    obj_bucket, _, obj_key = obj[k][len(\"s3://\"):].partition(\"/\")\n",
    "                    obj_bucket = BUCKET_MAPPINGS.get(obj_bucket, obj_bucket)\n",
    "                    for old_prefix in PREFIX_MAPPINGS:\n",
    "                        if obj_key.startswith(old_prefix):\n",
    "                            obj_key = (\n",
    "                                PREFIX_MAPPINGS[old_prefix]\n",
    "                                + obj_key[len(old_prefix):]\n",
    "                            )\n",
    "                    obj[k] = f\"s3://{obj_bucket}/{obj_key}\"\n",
    "                \n",
    "                # Find the job output field:\n",
    "                if source[\"job_name\"] in obj:\n",
    "                    source_label_attr = source[\"job_name\"]\n",
    "                elif standard_label_field in obj:\n",
    "                    source_label_attr = standard_label_field\n",
    "                else:\n",
    "                    raise RuntimeError(\"Couldn't find label field for entry in {}:\\n{}\".format(\n",
    "                        source[\"job_name\"],\n",
    "                        obj,\n",
    "                    ))\n",
    "                # Rename to standard:\n",
    "                obj[standard_label_field] = obj.pop(source_label_attr)\n",
    "                source_meta_attr = f\"{source_label_attr}-metadata\"\n",
    "                if source_meta_attr in obj:\n",
    "                    obj[f\"{standard_label_field}-metadata\"] = obj.pop(source_meta_attr)\n",
    "                # Write to output manifest:\n",
    "                fout.write(json.dumps(obj) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training and test sets\n",
    "\n",
    "To get some insight on how well our model is generalizing to real-world data, we'll need to reserve some annotated data as a testing/validation set.\n",
    "\n",
    "Below, we randomly partition the data into training and test sets and then upload the two manifests to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_manifest(f_in, f_train, f_test, train_pct=0.9, random_seed=1337):\n",
    "    logger.info(f\"Reading {f_in}\")\n",
    "    with open(f_in, \"r\") as fin:\n",
    "        lines = [l for l in filter(lambda l: l, fin)]\n",
    "    logger.info(f\"Shuffling records\")\n",
    "    random.Random(random_seed).shuffle(lines)\n",
    "    n_train = round(len(lines) * train_pct)\n",
    "\n",
    "    with open(f_train, \"w\") as ftrain:\n",
    "        logger.info(f\"Writing {n_train} records to {f_train}\")\n",
    "        for l in lines[:n_train]:\n",
    "            ftrain.write(l)\n",
    "    with open(f_test, \"w\") as ftest:\n",
    "        logger.info(f\"Writing {len(lines) - n_train} records to {f_test}\")\n",
    "        for l in lines[n_train:]:\n",
    "            ftest.write(l)\n",
    "\n",
    "split_manifest(\n",
    "    \"data/annotations/annotations-all.manifest.jsonl\",\n",
    "    \"data/annotations/annotations-train.manifest.jsonl\",\n",
    "    \"data/annotations/annotations-test.manifest.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_manifest_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/annotations/annotations-train.manifest.jsonl\"\n",
    "!aws s3 cp data/annotations/annotations-train.manifest.jsonl $train_manifest_s3uri\n",
    "\n",
    "test_manifest_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/annotations/annotations-test.manifest.jsonl\"\n",
    "!aws s3 cp data/annotations/annotations-test.manifest.jsonl $test_manifest_s3uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data\n",
    "\n",
    "Before training the model, we'll sense-check the data by plotting a few examples.\n",
    "\n",
    "The utility function below will overlay the page image with the annotated bounding boxes, the locations of `WORD` blocks detected from the Amazon Textract results, and the resulting classification of individual Textract `WORD`s.\n",
    "\n",
    "> ⏰ If you Textracted a large number of documents and haven't previously synced them to the notebook, the initial download here may take a few minutes to complete. For our sample set of 120, typically only ~20s is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!aws s3 sync --quiet $textract_s3uri ./data/textracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ **Note:** For the interactive visualization widgets in this notebook to work correctly, you'll need the [IPyWidgets extension for JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html).\n",
    ">\n",
    "> On [SageMaker Studio](https://aws.amazon.com/sagemaker/studio/), this should be installed by default.\n",
    ">\n",
    "> On the classic [SageMaker Notebook Instances](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html) though, you'll need to install the `@jupyter-widgets/jupyterlab-manager` extension (from `Settings > Extension Manager`, or using a [lifecycle configuration](https://docs.aws.amazon.com/sagemaker/latest/dg/notebook-lifecycle-config.html) similar to [this sample](https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/tree/master/scripts/install-lab-extension)) - or just use plain `Jupyter` instead of `JupyterLab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/annotations/annotations-test.manifest.jsonl\", \"r\") as fman:\n",
    "    test_examples = [json.loads(l) for l in filter(lambda l: l, fman)]\n",
    "\n",
    "util.viz.draw_from_manifest_items(\n",
    "    test_examples,\n",
    "    standard_label_field,\n",
    "    entity_classes,\n",
    "    imgs_s3uri[len(\"s3://\"):].partition(\"/\")[2],\n",
    "    textract_s3key_prefix=textract_s3uri[len(\"s3://\"):].partition(\"/\")[2],\n",
    "    imgs_local_prefix=\"data/imgs-clean\",\n",
    "    textract_local_prefix=\"data/textracted\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-supervised pre-training\n",
    "\n",
    "In many cases, businesses have a great deal more relevant *unlabelled* data available in addition to the manually labeled dataset. For example, you might have many more historical documents available (with OCR results already, or able to be processed with Amazon Textract) than you're reasonably able to annotate entities on - just as we do in this example!\n",
    "\n",
    "Large-scale language models like LayoutLM are typically **pre-trained** to unlabelled data in a **self-supervised** pattern: Teaching the model to predict some implicit task in the data like, for example, masking a few words on the page and predicting what words should go in the gaps.\n",
    "\n",
    "This pre-training doesn't directly teach the model to perform the target task (classifying entities), but forces the core of the model to learn intrinsic patterns in the data. When we then replace the output layers and **fine-tune** towards the target task with human-labelled data, the model is able to learn the target task more effectively.\n",
    "\n",
    "**In this example, pre-training is optional**:\n",
    "\n",
    "- By default, for speed, the configuration below will use a public pre-trained model from the [Hugging Face Transformers model repository](https://huggingface.co/models?search=layoutlm). This allows us to focus immediately on fine-tuning to our task; but also means accuracy may be degraded if our documents are very different from the original corpus the model was trained on.\n",
    "- Alternatively, set `pretrain = True` below to *further* pre-train this same base public model on your own Textracted documents first.\n",
    "\n",
    "Pre-training more likely to be valuable where you have a broader range of data available than the core supervised/annotated dataset, and the language/layouts used in your domain are unusual or specicalized. If you followed through [Notebook 1](1.%20Data%20Preparation.ipynb) with the default settings to Amazon Textract only a small sample of the documents, you may like to go back, increase `N_DOCS_KEPT`, and Textract some more of the source documents first.\n",
    "\n",
    "> ⚠️ **Note:** Refer to the [Amazon SageMaker Pricing Page](https://aws.amazon.com/sagemaker/pricing/) for up-to-date guidance before running large pre-training jobs.\n",
    ">\n",
    "> In our tests at the time of writing:\n",
    ">\n",
    "> - Pre-training on only the 120 \"sample\" documents to 25 epochs took about 30 minutes on an `ml.p3.8xlarge` instance with per-device batch size 4\n",
    "> - Pre-training on a larger 500-document subset with the same infrastructure and settings took about an hour\n",
    "> - Although the observed effect on downstream (entity recognition) accuracy metrics was generally positive in either case, it was small compared to variation over random seed initializations in fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain = False  # Set this True instead to run an additional pre-training job.\n",
    "\n",
    "pretrained_s3_uri = None  # Will be overwritten later if pretrain is enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For self-supervised pre-training, you can utilize the full available corpus of Textract-processed documents: Not just the subset of documents and pages you have annotations for. Reserving some documents for validation is still a good idea though, to understand if and when the model starts to over-fit.\n",
    "\n",
    "Arguably, including pages from the entity recognition validation dataset in pre-training constitutes [leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning)): Because even though we're not including any information about the entity labels the NER model will predict, we're teaching the model information about patterns of content in the hold-out pages.\n",
    "\n",
    "Therefore, the below code takes a conservative view to avoid possibly over-estimating the added benefits of pre-training: Constructing manifests to route *any document with pages in the entity recognition validation set* to also be in the validation set for pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfsup_train_manifest_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/docs-train.manifest.jsonl\"\n",
    "selfsup_val_manifest_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/docs-val.manifest.jsonl\"\n",
    "\n",
    "# To avoid information leakage, take the validation set = the set of all documents with *any* pages\n",
    "# mentioned in the validation set:\n",
    "val_textract_s3uris = set()\n",
    "with open(\"data/annotations/annotations-test.manifest.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        val_textract_s3uris.add(json.loads(line)[\"textract-ref\"])\n",
    "with open(\"data/docs-val.manifest.jsonl\", \"w\") as f:\n",
    "    for uri in val_textract_s3uris:\n",
    "        f.write(json.dumps({\"textract-ref\": uri}) + \"\\n\")\n",
    "print(f\"Added {len(val_textract_s3uris)} docs to pre-training validation set\")\n",
    "\n",
    "# Any Textracted docs not mentioned in validation can go to training:\n",
    "train_textract_s3uris = set()\n",
    "with open(\"data/textracted-all.manifest.jsonl\", \"r\") as fner:\n",
    "    with open(\"data/docs-train.manifest.jsonl\", \"w\") as f:\n",
    "        for line in fner:\n",
    "            uri = json.loads(line)[\"textract-ref\"]\n",
    "            if (uri in val_textract_s3uris) or (uri in train_textract_s3uris):\n",
    "                continue\n",
    "            else:\n",
    "                train_textract_s3uris.add(uri)\n",
    "                f.write(json.dumps({\"textract-ref\": uri}) + \"\\n\")\n",
    "print(f\"Added {len(train_textract_s3uris)} docs to pre-training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp data/docs-train.manifest.jsonl {selfsup_train_manifest_s3uri}\n",
    "!aws s3 cp data/docs-val.manifest.jsonl {selfsup_val_manifest_s3uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Amazon Textract JSONs prepared on S3 and split between training and validation via manifests, we're ready to run the pre-training.\n",
    "\n",
    "> ▶️ See the following *Fine-tuning on annotated data* section for more details and links on how model training works in SageMaker - which are omitted here since this section is optional.\n",
    "\n",
    "Since customized inputs for this job might be more variable than fine-tuning (because annotating data requires effort, but scaling up your unlabelled corpus may be easy), it's worth mentioning some relevant parameter options:\n",
    "\n",
    "- **`instance_type`**: While `ml.g4dn.xlarge` is a nice, low-hourly-cost, GPU-enabled option for our small data fine-tuning job later; the larger data volume in pre-training makes the speed-up available from `ml.p3.2xlarge` more significant. The provided script is multi-GPU capable, so for bigger jobs you may find `ml.p3.8xlarge` and beyond give more acceptable run-times.\n",
    "- **`per_device_train_batch_size`**: Controls *per-accelerator* batching; so bear in mind that moving up to a multi-GPU instance type (such as 4 GPUs in an `ml.p3.8xlarge`) implicitly increases the overall batch size for learning.\n",
    "- Other hyperparameters are available, as the implementation is generally based on the [Hugging Face TrainingArguments parser](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments) with [customizations applied in src/code/config.py](src/code/config.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    # (See src/code/config.py for more info on script parameters)\n",
    "    \"task_name\": \"mlm\",\n",
    "    \"textract_prefix\": textract_s3uri[len(\"s3://\"):].partition(\"/\")[2],\n",
    "\n",
    "    \"model_name_or_path\": \"microsoft/layoutlm-base-uncased\",\n",
    "    \n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "\n",
    "    \"num_train_epochs\": 25,\n",
    "    \"early_stopping_patience\": 10,\n",
    "    \"metric_for_best_model\": \"eval_loss\",\n",
    "    \"greater_is_better\": \"false\",\n",
    "    \n",
    "    # Early stopping implies checkpointing every evaluation (epoch), so limit the total checkpoints\n",
    "    # kept to avoid filling up disk:\n",
    "    \"save_total_limit\": 10,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "metric_definitions = [\n",
    "    { \"Name\": \"epoch\", \"Regex\": util.training.get_hf_metric_regex(\"epoch\") },\n",
    "    { \"Name\": \"learning_rate\", \"Regex\": util.training.get_hf_metric_regex(\"learning_rate\") },\n",
    "    { \"Name\": \"train:loss\", \"Regex\": util.training.get_hf_metric_regex(\"loss\") },\n",
    "    { \"Name\": \"validation:loss\", \"Regex\": util.training.get_hf_metric_regex(\"eval_loss\") },\n",
    "    {\n",
    "        \"Name\": \"validation:samples_per_sec\",\n",
    "        \"Regex\": util.training.get_hf_metric_regex(\"eval_samples_per_second\"),\n",
    "    },\n",
    "]\n",
    "\n",
    "pre_estimator = HuggingFaceEstimator(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"src\",\n",
    "    py_version=\"py38\",\n",
    "    pytorch_version=\"1.9\",\n",
    "    transformers_version=\"4.11\",\n",
    "\n",
    "    base_job_name=\"layoutlm-cfpb-pretrain\",\n",
    "    output_path=f\"s3://{bucket_name}/{bucket_prefix}trainjobs\",\n",
    "\n",
    "    instance_type=\"ml.p3.8xlarge\",\n",
    "    instance_count=1,\n",
    "    volume_size=50,\n",
    "\n",
    "    debugger_hook_config=False,\n",
    "#     profiler_config=sagemaker.debugger.ProfilerConfig(\n",
    "#         framework_profile_params=sagemaker.debugger.FrameworkProfile(),\n",
    "#     ),\n",
    "\n",
    "    hyperparameters=hyperparameters,\n",
    "    metric_definitions=metric_definitions,\n",
    "    # Required for our custom dataset loading code (which depends on tokenizer):\n",
    "    environment={ \"TOKENIZERS_PARALLELISM\": \"false\", },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if pretrain:\n",
    "    pre_estimator.fit(\n",
    "        inputs={\n",
    "            \"train\": selfsup_train_manifest_s3uri,\n",
    "            \"textract\": textract_s3uri + \"/\",\n",
    "            \"validation\": selfsup_val_manifest_s3uri,\n",
    "        },\n",
    "        #wait=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the pre-training is complete, fetch the output model S3 URI to use as input for the fine-tuning stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pretrain:\n",
    "    # Un-comment this first line to load an previous pre-training job instead:\n",
    "    # pre_estimator = HuggingFaceEstimator.attach(\"layoutlm-cfpb-pretrain-2021-11-17-01-53-05-786\")\n",
    "\n",
    "    pretraining_job_desc = pre_estimator.latest_training_job.describe()\n",
    "    pretrained_s3_uri = pretraining_job_desc[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "\n",
    "print(f\"Custom pre-trained model: {pretrained_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning on annotated data\n",
    "\n",
    "In this section we'll run a [SageMaker Training Job](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html) to fine-tune the model on our annotated dataset.\n",
    "\n",
    "In this process:\n",
    "\n",
    "- SageMaker will run the job on a dedicated, managed instance of type we choose (we'll use `ml.p*` or `ml.g*` GPU-accelerated types), allowing us to keep this notebook's resources modest and only pay for the seconds of GPU time the training job needs.\n",
    "- The data as specified in the manifest files will be downloaded from Amazon S3.\n",
    "- The bundle of scripts we provide (in `src/`) will be transparently uploaded to S3 and then run inside the specified SageMaker-provided [framework container](https://docs.aws.amazon.com/sagemaker/latest/dg/docker-containers-prebuilt.html). There's no need for us to build our own container image or implement a serving stack for inference (although fully-custom containers are [also supported](https://docs.aws.amazon.com/sagemaker/latest/dg/docker-containers.html)).\n",
    "- Job hyperparameters will be passed through to our `src/` scripts as CLI arguments.\n",
    "- SageMaker will analyze the logs from the job (i.e. `print()` or `logger` calls from our script) with the regular expressions specified in `metric_definitions`, to scrape structured timeseries metrics like loss and accuracy.\n",
    "- When the job finishes, the contents of the `model` folder in the container will be automatically tarballed and uploaded to a `model.tar.gz` in Amazon S3.\n",
    "\n",
    "Rather than orchestrating this process through the low-level [SageMaker API](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html) (e.g. via [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_training_job)), we'll use the open-source [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/) (`sagemaker`) for convenience.\n",
    "\n",
    "Rather than using the base [SageMaker PyTorch framework containers](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html), we'll take advantage of the [tailored containers for Hugging Face](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/index.html). You can also refer to [Hugging Face's own docs for training on SageMaker](https://huggingface.co/transformers/sagemaker.html) for more information, and of course the implementation of our training script here in the `src/` folder.\n",
    "\n",
    "First, we'll configure some parameters you may **sometimes wish to re-use across training jobs**. Continuation jobs may want to use the same checkpoint location in S3, while from-scratch training should start fresh\n",
    "\n",
    "▶️ You can choose when to re-run this cell between experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_collection_name = \"checkpoints-\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "print(f\"Saving checkpoints to collection {checkpoint_collection_name}\")\n",
    "\n",
    "checkpoint_s3_uri = f\"s3://{bucket_name}/{bucket_prefix}checkpoints/{checkpoint_collection_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define the core configuration for our training job:\n",
    "\n",
    "▶️ This should usually be re-run for every new training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    # (See src/code/config.py for more info on script parameters)\n",
    "    \"annotation_attr\": standard_label_field,\n",
    "    \"textract_prefix\": textract_s3uri[len(\"s3://\"):].partition(\"/\")[2],\n",
    "    \"num_labels\": len(fields) + 1,  # +1 for \"other\"\n",
    "\n",
    "    \"num_train_epochs\": 150,  # Set high for automatic HP tuning later\n",
    "    \"early_stopping_patience\": 5,  # Usually stops after <20 epochs on this sample data+config\n",
    "    \"metric_for_best_model\": \"eval_focus_else_acc_minus_one\",\n",
    "    \"greater_is_better\": \"true\",\n",
    "\n",
    "    # Early stopping implies checkpointing every evaluation (epoch), so limit the total checkpoints\n",
    "    # kept to avoid filling up disk:\n",
    "    \"save_total_limit\": 10,\n",
    "}\n",
    "if not pretrained_s3_uri:\n",
    "    hyperparameters[\"model_name_or_path\"] = \"microsoft/layoutlm-base-uncased\"\n",
    "\n",
    "def get_hf_metric_regex(metric_name):\n",
    "    \"\"\"Build RegEx string to extract a numeric HuggingFace Transformers metric from logs\n",
    "\n",
    "    HF metric log lines look like a Python dict print e.g:\n",
    "    {'eval_loss': 0.3940396010875702, ..., 'epoch': 1.0}\n",
    "    \"\"\"\n",
    "    scientific_number_exp = r\"(-?[0-9]+(\\.[0-9]+)?(e[+\\-][0-9]+)?)\"\n",
    "    return \"\".join((\n",
    "        \"'\",\n",
    "        metric_name,\n",
    "        \"': \",\n",
    "        scientific_number_exp,\n",
    "        \"[,}]\",\n",
    "    ))\n",
    "\n",
    "metric_definitions = [\n",
    "    { \"Name\": \"epoch\", \"Regex\": get_hf_metric_regex(\"epoch\") },\n",
    "    { \"Name\": \"learning_rate\", \"Regex\": get_hf_metric_regex(\"learning_rate\") },\n",
    "    { \"Name\": \"train:loss\", \"Regex\": get_hf_metric_regex(\"loss\") },\n",
    "    { \"Name\": \"validation:n_examples\", \"Regex\": get_hf_metric_regex(\"eval_n_examples\") },\n",
    "    { \"Name\": \"validation:loss_avg\", \"Regex\": get_hf_metric_regex(\"eval_loss\") },\n",
    "    { \"Name\": \"validation:acc\", \"Regex\": get_hf_metric_regex(\"eval_acc\") },\n",
    "    { \"Name\": \"validation:focus_acc\", \"Regex\": get_hf_metric_regex(\"eval_focus_acc\") },\n",
    "    { \"Name\": \"validation:target\", \"Regex\": get_hf_metric_regex(\"eval_focus_else_acc_minus_one\") },\n",
    "]\n",
    "\n",
    "estimator = HuggingFaceEstimator(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"src\",\n",
    "    py_version=\"py38\",\n",
    "    pytorch_version=\"1.9\",\n",
    "    transformers_version=\"4.11\",\n",
    "\n",
    "    base_job_name=\"layoutlm-cfpb-hf\",\n",
    "    output_path=f\"s3://{bucket_name}/{bucket_prefix}trainjobs\",\n",
    "    #checkpoint_s3_uri=checkpoint_s3_uri,  # Un-comment to turn on checkpoint upload to S3\n",
    "\n",
    "    instance_type=\"ml.g4dn.xlarge\",  # Could also consider ml.p3.2xlarge\n",
    "    instance_count=1,\n",
    "    volume_size=50,\n",
    "\n",
    "    #debugger_hook_config=False,\n",
    "    profiler_config=sagemaker.debugger.ProfilerConfig(\n",
    "        framework_profile_params=sagemaker.debugger.FrameworkProfile(),\n",
    "    ),\n",
    "\n",
    "    hyperparameters=hyperparameters,\n",
    "    metric_definitions=metric_definitions,\n",
    "    # Required for our custom dataset loading code (which depends on tokenizer):\n",
    "    environment={ \"TOKENIZERS_PARALLELISM\": \"false\", },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the below cell will actually kick off the training job and stream logs from the running container.\n",
    "\n",
    "> ℹ️ You'll also be able to check the status of the job in the [Training jobs page of the SageMaker Console](https://console.aws.amazon.com/sagemaker/home?#/jobs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"train\": train_manifest_s3uri,\n",
    "    \"textract\": textract_s3uri + \"/\",\n",
    "    \"validation\": test_manifest_s3uri,\n",
    "}\n",
    "if pretrained_s3_uri:\n",
    "    print(f\"Using custom pre-trained model {pretrained_s3_uri}\")\n",
    "    inputs[\"model_name_or_path\"] = pretrained_s3_uri\n",
    "\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Hyperparameter tuning\n",
    "\n",
    "Particularly when applying novel techniques or working in new domains, we'll often need to find good values for a range of different *hyperparameters* of our proposed algorithms.\n",
    "\n",
    "Rather than spending time manually adjusting these parameters, we can use [SageMaker Automatic Model Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html) which uses an intelligent [Bayesian optimization approach](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html) to efficiently and automatically search for high-performing combinations over several training jobs.\n",
    "\n",
    "You can optionally run the cell below to kick off an HPO job for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = sagemaker.tuner.HyperparameterTuner(\n",
    "    estimator,\n",
    "    \"validation:target\",\n",
    "    base_tuning_job_name=\"layoutlm-cfpb-hpo\",\n",
    "    hyperparameter_ranges={\n",
    "        \"learning_rate\": sagemaker.parameter.ContinuousParameter(\n",
    "            1e-8,\n",
    "            1e-3,\n",
    "            scaling_type=\"Logarithmic\",\n",
    "        ),\n",
    "        \"per_device_train_batch_size\": sagemaker.parameter.CategoricalParameter([2, 4, 6, 8]),\n",
    "        \"label_smoothing_factor\": sagemaker.parameter.CategoricalParameter([0.0, 1e-12, 1e-9, 1e-6]),\n",
    "    },\n",
    "    metric_definitions=metric_definitions,\n",
    "    strategy=\"Bayesian\",\n",
    "    objective_type=\"Maximize\",\n",
    "    max_jobs=21,\n",
    "    max_parallel_jobs=2,\n",
    "    #early_stopping_type=\"Auto\",  # Off by default - could consider turning it on\n",
    "#     warm_start_config=sagemaker.tuner.WarmStartConfig(\n",
    "#         warm_start_type=sagemaker.tuner.WarmStartTypes.IDENTICAL_DATA_AND_ALGORITHM,\n",
    "#         parents={ \"layoutlm-cfpb-hpo-210723-1625\" },\n",
    "#     ),\n",
    ")\n",
    "\n",
    "tuner.fit(\n",
    "    inputs={\n",
    "        \"train\": train_manifest_s3uri,\n",
    "        \"textract\": textract_s3uri + \"/\",\n",
    "        \"validation\": test_manifest_s3uri,\n",
    "    },\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This job will run asynchronously so won't block the notebook, but you can check on the status from the [Hyperparameter tuning jobs list](https://console.aws.amazon.com/sagemaker/home?#/hyper-tuning-jobs) of the SageMaker Console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model\n",
    "\n",
    "Once our model is trained (or maybe even automatically hyperparameter-tuned over several training jobs), it's ready to be deployed for real-time or batch inference.\n",
    "\n",
    "Note that if, for some reason, you need to recover the state of a previous training or tuning job after a notebook restart or similar, you can `attach()` to training or tuning jobs by name - as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, you can attach to a previous training job by name like this:\n",
    "# estimator = HuggingFaceEstimator.attach(\"layoutlm-cfpb-210529-0851-006-5ee95cde\")\n",
    "# tuner = sagemaker.tuner.HyperparameterTuner.attach(\"layoutlm-cfpb-hpo-210603-0542\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy one-click deployment\n",
    "\n",
    "For straightforward deployment, you can just call `estimator.deploy()` (or equivalently, `tuner.deploy()`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = estimator.latest_training_job.describe()[\"TrainingJobName\"]\n",
    "# Or:\n",
    "# training_job_name = tuner.best_training_job()\n",
    "\n",
    "predictor = estimator.deploy(\n",
    "    # Avoid us accidentally deploying the model twice by setting name per training job:\n",
    "    endpoint_name=training_job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    "    # TODO: Disable once debugging is done\n",
    "    env={ \"PYTHONUNBUFFERED\": \"1\" },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Digging deeper into the model\n",
    "\n",
    "Alternatively, you may instead want to explore the artifacts saved by the training job, or edit the `code` script bundle before deploying the endpoint - especially for debugging any problems with inference. Let's see how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smclient = boto3.client(\"sagemaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_desc = estimator.latest_training_job.describe()\n",
    "model_s3uri = training_job_desc[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "model_name = training_job_desc[\"TrainingJobName\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./data/model\n",
    "!aws s3 cp $model_s3uri ./data/model/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd data/model && tar -xzvf model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "try:\n",
    "    # Make sure we don't accidentally re-use same model:\n",
    "    smclient.delete_model(ModelName=model_name)\n",
    "    print(f\"Deleted existing model {model_name}\")\n",
    "except smclient.exceptions.ClientError as e:\n",
    "    if not (\n",
    "        e.response[\"Error\"][\"Code\"] in (404, \"404\")\n",
    "        or e.response[\"Error\"].get(\"Message\", \"\").startswith(\"Could not find model\")\n",
    "    ):\n",
    "        raise e\n",
    "\n",
    "model = HuggingFaceModel(\n",
    "    name=model_name,\n",
    "    model_data=model_s3uri,\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    source_dir=\"src/\",\n",
    "    entry_point=\"inference.py\",\n",
    "    transformers_version=\"4.11\",\n",
    "    py_version=\"py38\",\n",
    "    pytorch_version=\"1.9\",\n",
    "    # TODO: Disable once debugging is done\n",
    "    env={ \"PYTHONUNBUFFERED\": \"1\" },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Delete previous endpoint, if already in use:\n",
    "    predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "    print(\"Deleting previous endpoint...\")\n",
    "    time.sleep(8)\n",
    "except (NameError, smclient.exceptions.ResourceNotFound):\n",
    "    pass  # No existing endpoint to delete\n",
    "except smclient.exceptions.ClientError as e:\n",
    "    if \"Could not find\" not in e.response[\"Error\"].get(\"Message\", \"\"):\n",
    "        raise e\n",
    "\n",
    "\n",
    "print(\"Deploying model...\")\n",
    "predictor = model.deploy(\n",
    "    endpoint_name=training_job_desc[\"TrainingJobName\"],\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    "    # TODO: Disable once debugging is done\n",
    "    env={ \"PYTHONUNBUFFERED\": \"1\" },\n",
    ")\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Optimize costs with Asynchronous Inference\n",
    "\n",
    "In the examples above, we deployed the trained model to a [real-time inference endpoint](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html) on SageMaker. These real-time endpoints provide synchronous (request-response) inference, and can be configured to [auto-scale](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html) based on demand.\n",
    "\n",
    "However, for [SageMaker asynchronous inference](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html) may be a better fit for many document processing use-cases:\n",
    "\n",
    "1. Unlike real-time endpoints (at the time of writing), asynchronous endpoints can [auto-scale down to zero instances](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-autoscale.html). This can offer substantial cost savings if your business process is low-volume and often idle: With the trade-off that overall process latency may increase, especially for cold-start requests.\n",
    "1. Asynchronous inference can support longer timeouts and larger request/response payload sizes than real-time: Which can be useful in cases where an individual document may be long and take a significant time to process with a model.\n",
    "\n",
    "The OCR pipeline stack is configured to handle either synchronous or asynchronous endpoints by default, so you can alternatively set up an auto-scaling asynchronous endpoint as shown below and use that in the pipeline later.\n",
    "\n",
    "First, create the asynchronous endpoint:\n",
    "\n",
    "- As detailed in the [SDK docs](https://sagemaker.readthedocs.io/en/stable/overview.html#sagemaker-asynchronous-inference), the optional `async_inference_config` parameter tells SageMaker that the endpoint will be asynchronous rather than real-time.\n",
    "- For permissions integration, our async endpoint will need to store its outputs in the proper S3 location the pipeline is expecting (`output_path`). We can look that up from here in the notebook via the same SSM-based `config` we've seen before.\n",
    "- To resume the pipeline when the model processes a document, our endpoint will need to notify the pipeline's SNS topic. Again, this is given on `config`.\n",
    "- While the *SageMaker* limits on request/response size are higher for asynchronous endpoints than real-time, we need to also make sure the serving stack *within the container* is configured to support very large payloads. Setting the `MMS_MAX_REQUEST_SIZE` and `MMS_MAX_RESPONSE_SIZE` environment variables below prevents errors related to this. For more information see the [AWSLabs Multi-Model Server configuration doc](https://github.com/awslabs/multi-model-server/blob/master/docs/configuration.md) and corresponding page [for TorchServe](https://github.com/pytorch/serve/blob/master/docs/configuration.md#other-properties)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_async = estimator.deploy(\n",
    "    # Avoid us accidentally deploying the model twice by setting name per training job:\n",
    "    endpoint_name=\"async-\" + training_job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    "    env={\n",
    "        \"PYTHONUNBUFFERED\": \"1\",  # TODO: Disable once debugging is done\n",
    "        \"MMS_MAX_REQUEST_SIZE\": str(100*1024*1024),  # 100MiB instead of default ~6.2MiB\n",
    "        \"MMS_MAX_RESPONSE_SIZE\": str(100*1024*1024),  # 100MiB instead of default ~6.2MiB\n",
    "    },\n",
    "    async_inference_config=sagemaker.async_inference.AsyncInferenceConfig(\n",
    "        output_path=f\"s3://{config.model_results_bucket}\",\n",
    "        max_concurrent_invocations_per_instance=2,\n",
    "        notification_config={\n",
    "            \"SuccessTopic\": config.model_callback_topic_arn,\n",
    "            \"ErrorTopic\": config.model_callback_topic_arn,\n",
    "        },\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, [configure auto-scaling](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-autoscale.html) for the endpoint by first registering it with the [application auto-scaling service](https://docs.aws.amazon.com/autoscaling/application/userguide/what-is-application-auto-scaling.html) and then applying a scaling policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appscaling = boto3.client(\"application-autoscaling\")\n",
    "\n",
    "# Define and register your endpoint variant\n",
    "appscaling.register_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=f\"endpoint/{predictor_async.endpoint_name}/variant/AllTraffic\",\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    MinCapacity=0,  # (MinCapacity 0 not supported with real-time endpoints)\n",
    "    MaxCapacity=5,\n",
    ")\n",
    "print(f\"Endpoint registered with auto-scaling service: {predictor_async.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_policy_resp = appscaling.put_scaling_policy(\n",
    "    PolicyName=f\"sagemaker-endpoint-{predictor_async.endpoint_name}\",\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=f\"endpoint/{predictor_async.endpoint_name}/variant/AllTraffic\",\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    PolicyType=\"TargetTrackingScaling\",\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        \"TargetValue\": 5.0,\n",
    "        \"CustomizedMetricSpecification\": {\n",
    "            \"MetricName\": \"ApproximateBacklogSizePerInstance\",\n",
    "            \"Namespace\": \"AWS/SageMaker\",\n",
    "            \"Dimensions\": [\n",
    "                { \"Name\": \"EndpointName\", \"Value\": predictor_async.endpoint_name },\n",
    "            ],\n",
    "            \"Statistic\": \"Average\",\n",
    "        },\n",
    "        \"ScaleInCooldown\": 5 * 60,  # (seconds)\n",
    "        \"ScaleOutCooldown\": 2 * 60,  # (seconds)\n",
    "    }\n",
    ")\n",
    "print(f\"Created/updated scaling policy ARN:\\n{scaling_policy_resp['PolicyARN']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed, the policy can be later deleted and the endpoint de-registered from auto-scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appscaling.delete_scaling_policy(\n",
    "#     PolicyName=f\"sagemaker-endpoint-{predictor_async.endpoint_name}\",\n",
    "#     ServiceNamespace=\"sagemaker\",\n",
    "#     ResourceId=f\"endpoint/{predictor_async.endpoint_name}/variant/AllTraffic\",\n",
    "#     ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "# )\n",
    "# print(f\"Auto-scaling policy deleted for endpoint {predictor_async.endpoint_name}\")\n",
    "\n",
    "# appscaling.deregister_scalable_target(\n",
    "#     ServiceNamespace=\"sagemaker\"\n",
    "#     ResourceId=f\"endpoint/{predictor_async.endpoint_name}/variant/AllTraffic\",\n",
    "#     ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "# )\n",
    "# print(f\"Auto-scaling de-registered from endpoint {predictor_async.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Model\n",
    "\n",
    "Once the deployment is complete, we're ready to try it out with some real-time requests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As with estimators, you can attach the notebook to a previously deployed endpoint like this:\n",
    "# from sagemaker.huggingface import HuggingFacePredictor\n",
    "# predictor = HuggingFacePredictor(\n",
    "#     \"layoutlm-cfpb-hf-2021-09-02-01-08-11-234\",\n",
    "#     serializer=sagemaker.serializers.JSONSerializer(),\n",
    "#     deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making requests and rendering results\n",
    "\n",
    "This model accepts Textract-like JSON (e.g. as returned by [AnalyzeDocument](https://docs.aws.amazon.com/textract/latest/dg/API_AnalyzeDocument.html#API_AnalyzeDocument_ResponseSyntax) or [DetectDocumentText](https://docs.aws.amazon.com/textract/latest/dg/API_DetectDocumentText.html#API_DetectDocumentText_ResponseSyntax) APIs) and classifies each `WORD` [block](https://docs.aws.amazon.com/textract/latest/dg/API_Block.html) according to the entity classes we defined earlier: Returning the same JSON with additional fields added to indicate the predictions.\n",
    "\n",
    "We can use utility functions to render these predictions as we did the manual annotations previously:\n",
    "\n",
    "> ⚠️ **Note:** For long multi-page documents these JSON objects could become very large (many-MB), which can cross scaling thresholds such as configured payload limits (from SageMaker and/or TorchServe), inference timeouts (from SageMaker and/or TorchServe) or available memory.\n",
    ">\n",
    "> The handling logic in [src/code/inference.py](src/code/inference.py) supports a range of workarounds for this such as specific page selection and passing input and/or output by S3 reference instead of inline - as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import trp\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "def predict_from_manifest_item(\n",
    "    item,\n",
    "    predictor,\n",
    "    imgs_s3key_prefix=imgs_s3uri[len(\"s3://\"):].partition(\"/\")[2],\n",
    "    textract_s3key_prefix=textract_s3uri[len(\"s3://\"):].partition(\"/\")[2],\n",
    "    imgs_local_prefix=\"data/imgs-clean\",\n",
    "    textract_local_prefix=\"data/textracted\",\n",
    "    draw=True,\n",
    "):\n",
    "    paths = util.viz.local_paths_from_manifest_item(\n",
    "        item,\n",
    "        imgs_s3key_prefix,\n",
    "        textract_s3key_prefix=textract_s3key_prefix,\n",
    "        imgs_local_prefix=imgs_local_prefix,\n",
    "        textract_local_prefix=textract_local_prefix,\n",
    "    )\n",
    "\n",
    "    ## Basic inline request/response may fail for large, multi-page documents (because of breaking\n",
    "    ## the 5MB real-time inference payload limit; or the model running out of memory):\n",
    "#     with open(paths[\"textract\"], \"r\") as ftextract:\n",
    "#         result_json = predictor.predict(json.loads(ftextract.read()))\n",
    "\n",
    "    ## We can strip the JSON down to only the target page of interest like this:\n",
    "#     with open(paths[\"textract\"], \"r\") as ftextract:\n",
    "#         result_json = predictor.predict({\n",
    "#             \"Blocks\": trp.Document(\n",
    "#                 json.loads(ftextract.read()),\n",
    "#             ).pages[item[\"page-num\"] - 1].blocks\n",
    "#         })\n",
    "\n",
    "    ## Or have the model refer directly to S3 and return us only the page of interest:\n",
    "    result_json = predictor.predict({\n",
    "        \"S3Input\": { \"URI\": item[\"textract-ref\"] },\n",
    "        \"TargetPageNum\": item[\"page-num\"],\n",
    "        \"TargetPageOnly\": True,\n",
    "    })\n",
    "\n",
    "    ## If we wanted, we could even have the model save results to S3 and fetch them ourselves:\n",
    "    ## (Which is what the OCR pipeline does when configured with a real-time endpoint)\n",
    "#     result_json = predictor.predict({\n",
    "#         \"S3Input\": { \"URI\": item[\"textract-ref\"] },\n",
    "#         \"TargetPageNum\": item[\"page-num\"],\n",
    "#         \"TargetPageOnly\": True,\n",
    "#         \"S3Output\": { \"Bucket\": bucket_name, \"Key\": f\"{bucket_prefix}tmp/model-result\" },\n",
    "#     })\n",
    "#     result_json = json.loads(\n",
    "#         s3.Bucket(result_json[\"Bucket\"]).Object(result_json[\"Key\"]).get()[\"Body\"].read()\n",
    "#     )\n",
    "\n",
    "    result_trp = trp.Document(result_json)\n",
    "\n",
    "    if draw:\n",
    "        util.viz.draw_smgt_annotated_page(\n",
    "            paths[\"image\"],\n",
    "            entity_classes,\n",
    "            annotations=[],\n",
    "            textract_result=result_trp,\n",
    "            # Note that page_num should be item[\"page-num\"] if we requested the full set of pages\n",
    "            # from the model above:\n",
    "            page_num=1,\n",
    "        )\n",
    "    return result_trp\n",
    "\n",
    "\n",
    "widgets.interact(\n",
    "    lambda ix: predict_from_manifest_item(\n",
    "        test_examples[ix],\n",
    "        predictor,\n",
    "    ),\n",
    "    ix=widgets.IntSlider(\n",
    "        min=0,\n",
    "        max=len(test_examples) - 1,\n",
    "        step=1,\n",
    "        value=0,\n",
    "        description=\"Example:\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From token classification to entity detection\n",
    "\n",
    "You may have noticed a slight mismatch: We're talking about extracting 'fields' or 'entities' from the document, but our model just classifies individual words. Going from words to entities assumes we're able to understand which words go \"together\" and what order they should be read in.\n",
    "\n",
    "Fortunately, Textract helps us out with this too as the word blocks are already collected into `LINE`s.\n",
    "\n",
    "For many straightforward applications, we can simply loop through the lines on a page and define an \"entity detection\" as a contiguous group of the same class - as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = predict_from_manifest_item(\n",
    "    test_examples[6],\n",
    "    predictor,\n",
    "    draw=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_cls = len(entity_classes)\n",
    "prev_cls = other_cls\n",
    "current_entity = \"\"\n",
    "\n",
    "for page in res.pages:\n",
    "    for line in page.lines:\n",
    "        for word in line.words:\n",
    "            pred_cls = word._block[\"PredictedClass\"]\n",
    "            if pred_cls != prev_cls:\n",
    "                if prev_cls != other_cls:\n",
    "                    print(f\"----------\\n{entity_classes[prev_cls]}:\\n{current_entity}\")\n",
    "                prev_cls = pred_cls\n",
    "                if pred_cls != other_cls:\n",
    "                    current_entity = word.text\n",
    "                else:\n",
    "                    current_entity = \"\"\n",
    "                continue\n",
    "            current_entity = \" \".join((current_entity, word.text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course there may be some instances where this heuristic breaks down, but we still have access to all the position (and text) information from each `LINE` and `WORD` to write additional rules for reading order and separation if wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating the model with the OCR Pipeline\n",
    "\n",
    "If you've deployed the **OCR pipeline stack** in your AWS Account, you can now configure it to use this endpoint as follows:\n",
    "\n",
    "- First, identify the **endpoint name** of your deployed model. Assuming you created the predictor as above, you can simply run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, identify the **AWS Systems Manager Parameter** that configures the SageMaker endpoint for the OCR pipeline stack.\n",
    "\n",
    "The below code should pull it through for you, but alternatively you can refer to your stack's **Outputs** in the [AWS CloudFormation Console](https://console.aws.amazon.com/cloudformation/home?#/stacks). The Output name should include `SageMakerEndpoint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.sagemaker_endpoint_name_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, we'll update this SSM parameter to point to the deployed SageMaker endpoint.\n",
    "\n",
    "The below code should do this for you automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_endpoint_name = predictor.endpoint_name\n",
    "# Or, if you deployed a SageMaker async endpoint to use instead:\n",
    "#pipeline_endpoint_name = predictor_async.endpoint_name\n",
    "\n",
    "print(f\"Configuring pipeline with model: {pipeline_endpoint_name}\")\n",
    "\n",
    "ssm = boto3.client(\"ssm\")\n",
    "ssm.put_parameter(\n",
    "    Name=config.sagemaker_endpoint_name_param,\n",
    "    Overwrite=True,\n",
    "    Value=pipeline_endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you could open the [AWS Systems Manager Parameter Store console](https://console.aws.amazon.com/systems-manager/parameters/?tab=Table) and click on the *name* of the parameter to open its detail page, then the **Edit** button in the top right corner as shown below:\n",
    "\n",
    "![](img/ssm-param-detail-screenshot.png \"Screenshot of SSM parameter detail page showing Edit button\")\n",
    "\n",
    "From this screen you can manually set the **Value** of the parameter and save the changes.\n",
    "\n",
    "Whether you updated the SSM parameter via code or the console, your stack is now configured to use the deployed model for OCR enrichment!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the pipeline entity definitions\n",
    "\n",
    "As well as configuring the *enrichment* stage of the pipeline to reference the deployed version of the model, we need to configure the *post-processing* stage to match the model's **definition of entity/field types**.\n",
    "\n",
    "The entity configuration is as we saved in the previous notebook, but the `annotation_guidance` attributes are not needed:\n",
    "\n",
    "> ℹ️ **Note:** As well as the mapping from ID numbers (returned by the model) to human-readable class names, this configuration controls how the pipeline consolidates entity matches into \"fields\" of the document: E.g. choosing the \"most likely\" or \"first\" value between multiple detections, or setting up a multi-value field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_entity_config = json.dumps([f.to_dict(omit=[\"annotation_guidance\"]) for f in fields], indent=2)\n",
    "print(pipeline_entity_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, you *could* set this value manually in the SSM console for the parameter named as `EntityConfig`.\n",
    "\n",
    "...But we can make the same update via code through the APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Setting pipeline entity configuration\")\n",
    "ssm.put_parameter(\n",
    "    Name=config.entity_config_param,\n",
    "    Overwrite=True,\n",
    "    Value=pipeline_entity_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out the pipeline\n",
    "\n",
    "To see the pipeline in action:\n",
    "\n",
    "▶️ **Open** the [AWS Step Functions Console](https://console.aws.amazon.com/states/home?#/statemachines) and click on the name of your *State Machine* from the list to see its details.\n",
    "\n",
    "(If you can't find it in the list, the code below should look it up for you or you can check the *Outputs* tab of your pipeline stack in the [AWS CloudFormation Console](https://console.aws.amazon.com/cloudformation/home?#/stacks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Your pipeline state machine is:\")\n",
    "print(config.pipeline_sfn_arn.rpartition(\":\")[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▶️ **Locate** your pipeline's `InputBucket` in [Amazon S3](https://s3.console.aws.amazon.com/s3/home?)\n",
    "\n",
    "(Likewise you can look this up from CloudFormation or using the below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Your pipeline's input S3 bucket:\")\n",
    "print(config.pipeline_input_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▶️ **Upload** a sample document (PDF) from our dataset to the S3 bucket\n",
    "\n",
    "You can do this by dragging and dropping the file to the S3 console - or running the cells below to upload a test document through the AWS CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfpaths = []\n",
    "for currpath, dirs, files in os.walk(\"data/raw\"):\n",
    "    if \"/.\" in currpath or \"__\" in currpath:\n",
    "        continue\n",
    "    pdfpaths += [\n",
    "        os.path.join(currpath, f) for f in files\n",
    "        if f.lower().endswith(\".pdf\")\n",
    "    ]\n",
    "pdfpaths = sorted(pdfpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filepath = pdfpaths[0]\n",
    "test_s3uri = f\"s3://{config.pipeline_input_bucket_name}/{test_filepath}\"\n",
    "\n",
    "!aws s3 cp '{test_filepath}' '{test_s3uri}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that a new *execution* (run) of the state machine is triggered automatically:\n",
    "\n",
    "> ℹ️ This may take a few seconds after the upload is complete. If you're not seeing it:\n",
    ">\n",
    "> - Check you're in the correct \"pipeline\" state machine, as this solution's stack creates more than one state machine\n",
    "> - Try refreshing the page or the execution list\n",
    "\n",
    "![](img/sfn-statemachine-screenshot.png \"Screenshot of AWS Step Functions state machine detail page showing execution list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clicking through to the execution, you'll be able to see the progress through the workflow and output/error information.\n",
    "\n",
    "Depending on your configuration, your view may look a little different to the below and you may have **either a successful execution or a failure at the review step**:\n",
    "\n",
    "![](img/sfn-execution-status-screenshot.png \"Screenshot of Step Functions execution detail view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "You should now have been able to train and deploy the enrichment model, and demonstrate its integration to the pipeline.\n",
    "\n",
    "However, the final human review stage is not fully set up yet, so may have triggered an error.\n",
    "\n",
    "In the final notebook, we'll configure the human review functionality to finish up the flow: Open up **notebook [3. Human Review.ipynb](3.%20Human%20Review.ipynb)** to follow along.\n",
    "\n",
    "\n",
    "### A note on clean-up\n",
    "\n",
    "Note that while training, processing and transform jobs in SageMaker start and stop compute resources for the specific job being executed, deployed **endpoints** stay active (and therefore accumulating charges) until you turn them off.\n",
    "\n",
    "When you're finished using an endpoint, you should delete it either through the [Amazon SageMaker Console](https://console.aws.amazon.com/sagemaker/home?#/endpoints) or via commands like the below.\n",
    "\n",
    "(Of course, our OCR pipeline stack will throw an error if you try to run it configured with an Endpoint Name that no longer exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor_async.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-1:492261229750:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
