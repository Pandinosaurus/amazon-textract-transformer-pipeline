{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "525c8bba-5198-46b4-aad2-b6c4f51e6371",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Post-Processing Amazon Textract with Location-Aware Transformers**\n",
    "\n",
    "# Optional Extras\n",
    "\n",
    "> *This notebook works well with the `Data Science 1.0 / 2.0 (Python 3)` kernels on SageMaker Studio - use the same as for NB1*\n",
    "\n",
    "This notebook discusses optional extra/alternative steps separate from the typical pipeline setup flow. You won't typically need to run these steps unless specifically instructed.\n",
    "\n",
    "## Common setup\n",
    "\n",
    "First, as usual, we'll set up and import required libraries. You should run these cells regardless of which optional section(s) you're using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01620969-ea29-46c7-ba84-9006ca9e73d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker-studio-image-build \"sagemaker>=2.87,<3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fa81a6-6466-4a78-b87b-4bcec14ca983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "from logging import getLogger\n",
    "import os\n",
    "import time\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3  # General-purpose AWS SDK for Python\n",
    "import sagemaker  # High-level Python SDK for Amazon SageMaker\n",
    "\n",
    "# Local Dependencies:\n",
    "import util\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "# Configuration:\n",
    "bucket_name = sagemaker.Session().default_bucket()\n",
    "bucket_prefix = \"textract-transformers/\"\n",
    "print(f\"Working in bucket s3://{bucket_name}/{bucket_prefix}\")\n",
    "config = util.project.init(\"ocr-transformers-demo\")\n",
    "print(config)\n",
    "\n",
    "# AWS service clients:\n",
    "ssm = boto3.client(\"ssm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2939bcb1-bdce-48f2-83d7-c0e14cd6adea",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "The sections of this notebook are independent:\n",
    "\n",
    "- **[Manual thumbnail generator setup](#Manual-thumbnail-generator-setup)**: Customise online page thumbnail generation endpoint\n",
    "- **[Optimise costs with endpoint auto-scaling](#Optimise-costs-with-endpoint-auto-scaling)**: Configure your SageMaker endpoint(s) to auto-scale based on incoming request volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90d7dbd-95b3-42ab-a9f1-906742a6c860",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Manual thumbnail generator setup\n",
    "\n",
    "> This section walks through manually building and configuring the endpoint to generate resized page thumbnail images in real time.\n",
    ">\n",
    "> You may find it useful if you want to customise the container image or script used by this process, or if you deployed your pipeline without thumbnailing support but want to experiment with image-based models from notebooks.\n",
    ">\n",
    "> ⚠️ **Note:** Deploying and registering a thumbnailing endpoint from the notebook will still not turn on thumbnail generation in a pipeline deployed without support for it. Instead, refer to your CDK app parameters to ensure the pipeline state machine gets updated to include a thumbnail generation step.\n",
    "\n",
    "### Build and register custom container image\n",
    "\n",
    "The tools we use to read PDF files aren't installed by default in the pre-built SageMaker containers and aren't `pip install`able, so the thumbnail generator will need a custom container image. We can derive a custom image from an existing AWS DLC serving container, to minimise boilerplate code because a SageMaker-compatible serving stack will already be included.\n",
    "\n",
    "Because SageMaker Studio kernels are already containerized, you won't be able to run typical `docker build` commands you may be used to: So we'll use the [SageMaker Studio Image Build CLI](https://github.com/aws-samples/sagemaker-studio-image-build-cli) to build the image and store it in your account's [Amazon Elastic Container Registry (ECR)](https://aws.amazon.com/ecr/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f48a7b-cdf4-4f8a-9544-6942fbe582a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations:\n",
    "preproc_ecr_repo_name = \"sm-ocr-preproc\"\n",
    "preproc_ecr_image_tag = \"pytorch-1.10-inf-cpu\"\n",
    "\n",
    "preproc_framework_version = \"1.10\"\n",
    "preproc_py_version = \"py38\"\n",
    "\n",
    "base_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=os.environ[\"AWS_REGION\"],\n",
    "    instance_type=\"ml.c5.xlarge\",  # (Just used to check whether GPUs/accelerators are used)\n",
    "    py_version=preproc_py_version,\n",
    "    image_scope=\"inference\",  # Inference base because we'll also deploy as an endpoint later\n",
    "    version=preproc_framework_version,\n",
    ")\n",
    "\n",
    "# Combine together into the final URI (not needed for the build, but used later in the notebook):\n",
    "account_id = sagemaker.Session().account_id()\n",
    "region = os.environ[\"AWS_REGION\"]\n",
    "preproc_ecr_image_uri = \"{}.dkr.ecr.{}.amazonaws.com/{}:{}\".format(\n",
    "    account_id, region, preproc_ecr_repo_name, preproc_ecr_image_tag\n",
    ")\n",
    "print(f\"Will build to {preproc_ecr_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2d84a6-1b5b-4ebc-b057-9ffec1cb5fd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# (No need to re-run this cell if your image is already in ECR)\n",
    "\n",
    "# Actually build & push the container image:\n",
    "!cd custom-containers/preproc && sm-docker build . \\\n",
    "    --repository {ecr_repo_name}:{ecr_image_tag} \\\n",
    "    --role {config.sm_image_build_role} \\\n",
    "    --build-arg BASE_IMAGE={base_image_uri}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc24da6-d3b8-4b13-ab94-fd7085836678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check from notebook whether the image was successfully created:\n",
    "ecr = boto3.client(\"ecr\")\n",
    "imgs_desc = ecr.describe_images(\n",
    "    registryId=account_id,\n",
    "    repositoryName=preproc_ecr_repo_name,\n",
    "    imageIds=[{\"imageTag\": preproc_ecr_image_tag}],\n",
    ")\n",
    "assert len(imgs_desc[\"imageDetails\"]) > 0, \"Couldn't find ECR image {} after build\".format(\n",
    "    preproc_ecr_image_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca987aff-e81d-46e6-a723-4f7e980481cc",
   "metadata": {},
   "source": [
    "### Deploy and test the thumbnailer endpoint\n",
    "\n",
    "Because the custom image is based on the standard SageMaker PyTorch inference container, our [preproc/preproc.py](preproc/preproc.py) script can [work with the existing serving stack](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#id3) by exposing custom `model_fn`, `input_fn`, `predict_fn`, and/or `output_fn` functions.\n",
    "\n",
    "We'll bundle the scripts into a `.tar.gz` file in the format the PyTorch container expects: With inference code in a `code/` subfolder.\n",
    "\n",
    "Normally this process (and the setting of the `SAGEMAKER_PROGRAM` and `SAGEMAKER_SUBMIT_DIRECTORY` environment variables) is handled automatically by the `PyTorchModel` - which allows \"re-packing\" the tarball from a training job to create a new tarball with new `source_dir` and `entry_point` scripts. In this case though, we don't need such a two-step process because there's no training artifact to start from and no actual \"model\" in this tarball - PyTorch or otherwise. Our script just defines code to extract and resize page images, and a dummy `model_fn` so the endpoint won't crash from failing to find a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7cecb5-9b7d-41d8-ad5b-c54c7ebc2c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress the archive locally and list the compressed contents:\n",
    "preproc_model_path = util.deployment.tar_as_inference_code(\"preproc\", \"data/preproc-model.tar.gz\")\n",
    "print(f\"(Re)-created {preproc_model_path}\")\n",
    "!tar -ztvf {preproc_model_path}\n",
    "print()\n",
    "\n",
    "# Upload to S3:\n",
    "preproc_model_key = \"\".join((\n",
    "    bucket_prefix,\n",
    "    \"preproc-model/\",\n",
    "    util.uid.append_timestamp(\"model\"),  # (Maintain history in S3)\n",
    "    \".tar.gz\"\n",
    "))\n",
    "preproc_model_s3uri = f\"s3://{bucket_name}/{preproc_model_key}\"\n",
    "!aws s3 cp {preproc_model_path} {preproc_model_s3uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd15056b-2516-4e82-8464-3b9eee33421d",
   "metadata": {},
   "source": [
    "Once a `model.tar.gz` is available on S3, we're ready to create and deploy a SageMaker \"Model\" and Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443e9614-a7d0-404f-977b-12c848f7a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "if config.thumbnails_callback_topic_arn.startswith(\"arn:\"):\n",
    "    async_notification_config = {\n",
    "        \"SuccessTopic\": config.thumbnails_callback_topic_arn,\n",
    "        \"ErrorTopic\": config.thumbnails_callback_topic_arn,\n",
    "    }\n",
    "else:\n",
    "    logger.warning(\"Pipeline stack deployed without thumbnailing callback topic\")\n",
    "    async_notification_config = {}\n",
    "\n",
    "\n",
    "class PatchedPyTorchModel(PyTorchModel):\n",
    "    \"\"\"Modified PyTorchModel to allow manually setting SM Script Mode environment vars\n",
    "\n",
    "    See: https://github.com/aws/sagemaker-python-sdk/issues/3361\n",
    "    \"\"\"\n",
    "\n",
    "    def prepare_container_def(self, *args, **kwargs):\n",
    "        # Call the parent function:\n",
    "        result = super().prepare_container_def(*args, **kwargs)\n",
    "        # ...But allow our manual env vars configuration to override the internals:\n",
    "        manual_env = dict(self.env)\n",
    "        result[\"Environment\"].update(manual_env)\n",
    "        return result\n",
    "\n",
    "\n",
    "preproc_model = PatchedPyTorchModel(\n",
    "    name=util.uid.append_timestamp(\"ocr-thumbnail\"),\n",
    "    model_data=preproc_model_s3uri,\n",
    "    entry_point=None,  # Set manually via tarball and SAGEMAKER_PROGRAM\n",
    "    framework_version=\"1.10\",\n",
    "    py_version=\"py38\",\n",
    "    image_uri=preproc_ecr_image_uri,\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    env={\n",
    "        \"PYTHONUNBUFFERED\": \"1\",\n",
    "        \"MMS_MAX_REQUEST_SIZE\": str(100*1024*1024),  # 100MiB instead of default ~6.2MiB\n",
    "        \"MMS_MAX_RESPONSE_SIZE\": str(100*1024*1024),  # 100MiB instead of default ~6.2MiB\n",
    "        \"SAGEMAKER_PROGRAM\": \"preproc.py\",\n",
    "    },\n",
    ")\n",
    "\n",
    "preproc_predictor = preproc_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    async_inference_config=sagemaker.async_inference.AsyncInferenceConfig(\n",
    "        output_path=f\"s3://{config.model_results_bucket}/preproc\",\n",
    "        max_concurrent_invocations_per_instance=2,\n",
    "        notification_config=async_notification_config,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f573dd1-bae7-430f-b5ca-39d7c372c061",
   "metadata": {},
   "source": [
    "This endpoint accepts images or documents and outputs resized page thumbnail images.\n",
    "\n",
    "For multi-page documents the main output format is `application/x-npz`, which produces a [compressed numpy archive](https://numpy.org/doc/stable/reference/generated/numpy.savez_compressed.html#numpy.savez_compressed) in which `images` is an **array of images** each represented by **PNG bytes**. These formats require customizing the client (predictor) *serializer* and *deserializer* from the default for PyTorch. Since `Predictor` de/serializers set the `Content-Type` and `Accept` headers, we'll also need to re-configure the serializer whenever switching between input document types (for example PDF vs PNG).\n",
    "\n",
    "To support potentially large documents, the preprocessor is deployed to an **asynchronous** endpoint which enables larger request and response payload sizes.\n",
    "\n",
    "So how would it look to test the endpoint from Python? Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9bfda1-ffce-423c-8bee-d6cf761ece45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Choose an input (document or image):\n",
    "input_file = \"data/raw/121 Financial Credit Union/Visa Credit Card Agreement.pdf\"\n",
    "#input_file = \"data/imgs-clean/121 Financial Credit Union/Visa Credit Card Agreement-0001-1.png\"\n",
    "\n",
    "# Ensure de/serializers are correctly set up:\n",
    "preproc_predictor.serializer = util.deployment.FileSerializer.from_filename(input_file)\n",
    "preproc_predictor.deserializer = util.deployment.CompressedNumpyDeserializer()\n",
    "# Duplication because of https://github.com/aws/sagemaker-python-sdk/issues/3100\n",
    "preproc_predictor.predictor.serializer = preproc_predictor.serializer\n",
    "preproc_predictor.predictor.deserializer = preproc_predictor.deserializer\n",
    "\n",
    "# Run prediction:\n",
    "print(\"Calling endpoint...\")\n",
    "resp = preproc_predictor.predict(input_file)\n",
    "print(f\"Got response of type {type(resp)}\")\n",
    "\n",
    "# Render result:\n",
    "util.viz.draw_thumbnails_response(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2f0719-7314-4097-82f8-98169036e86c",
   "metadata": {},
   "source": [
    "### Connect thumbnailer to the deployed processing pipeline\n",
    "\n",
    "Once your thumbnailer endpoint is deployed and working, you can connect it into your document processing pipeline via SSM parameter configuration - just like the main enrichment model. This will only have an effect if your pipeline was already deployed with thumbnailing enabled, so the cell below will first check whether that seems to be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ef9eb-e39e-41fe-9309-78edae0375c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.thumbnails_callback_topic_arn == \"undefined\":\n",
    "    raise ValueError(\n",
    "        \"This pipeline CDK stack was deployed with thumbnailing disabled (by setting parameter \"\n",
    "        \"use_thumbnails=False). Either redeploy the CDK stack with updated settings to enable \"\n",
    "        \"thumbnailing, or continue without (and consider deleting the thumbnailing endpoint you \"\n",
    "        \"created, to save unnecessary cost).\"\n",
    "    )\n",
    "\n",
    "thumbnail_endpoint_name = preproc_predictor.endpoint_name\n",
    "print(f\"Configuring pipeline with thumbnailer: {thumbnail_endpoint_name}\")\n",
    "\n",
    "ssm.put_parameter(\n",
    "    Name=config.thumbnail_endpoint_name_param,\n",
    "    Overwrite=True,\n",
    "    Value=thumbnail_endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7972cc5-8025-46df-9661-143a47d8c8da",
   "metadata": {},
   "source": [
    "### Clean up experimental models\n",
    "\n",
    "Clean up any endpoints you created that are no longer required, to free up resources and avoid unnecessary ongoing costs. The below code demonstrates how to delete an endpoint, and its associated configuration & model records. you may also like to clean up the `preproc-model/` S3 folder to remove any old draft versions.\n",
    "\n",
    "> ⚠️ **Note:** If you delete the active endpoint/model your deployed pipeline is configured to use for thumbnailing, your pipeline will fail to process new documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c185e93-fc9f-451d-ab3d-d597665ea4a4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*[Back to contents](#Contents)*\n",
    "\n",
    "## Optimise costs with endpoint auto-scaling\n",
    "\n",
    "> This section demonstrates how you can enable and customise auto-scaling on your SageMaker endpoints to optimise resource use and cost.\n",
    ">\n",
    "> **Note:** For endpoints automatically deployed by the pipeline stack (such as the thumbnail generator), there are options available to configure this directly in CDK - which you may prefer.\n",
    "\n",
    "SageMaker Async Inference endpoints support [auto-scaling down to zero instances](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-autoscale.html) when not in use, which can provide significant cost-savings for use cases where document processing is occasional and the pipeline is often idle.\n",
    "\n",
    "⏰ **However:** You should be aware that enabling scale-to-zero can introduce cold-start delays of **several minutes** if requests arrive when all instances backing your endpoint have been shut down.\n",
    "\n",
    "### Setting up auto-scaling\n",
    "\n",
    "You can configure auto-scaling for your endpoint(s) by first registering them with the [application auto-scaling service](https://docs.aws.amazon.com/autoscaling/application/userguide/what-is-application-auto-scaling.html) and then applying a scaling policy as shown in the following cells.\n",
    "\n",
    "First, configure which SageMaker endpoint you want to auto-scale by name. SageMaker endpoints may be backed by multiple [variants](https://docs.aws.amazon.com/sagemaker/latest/dg/model-ab-testing.html) which can scale independently, but this sample only typically uses the default \"AllTraffic\" variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed66fc-8f53-43b5-9cdb-b5014620c621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, maybe you want to configure whichever enrichment model is currently in pipeline:\n",
    "endpoint_name = ssm.get_parameter(\n",
    "    Name=config.sagemaker_endpoint_name_param,\n",
    ")[\"Parameter\"][\"Value\"]\n",
    "\n",
    "# Default variant name unless you know otherwise:\n",
    "variant_name = \"AllTraffic\"\n",
    "\n",
    "print(f\"Configuring endpoint name:\\n  {endpoint_name}\")\n",
    "print(f\"Configuring variant name:\\n  {variant_name}\")\n",
    "\n",
    "resource_id = f\"endpoint/{endpoint_name}/variant/{variant_name}\"\n",
    "print(f\"\\nAuto-scaling resource ID:\\n  {resource_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd8a102-b4ba-43d8-9d74-9be06f7b00bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"ocr-thumbnail-2022-10-14-03-37-58-529\"\n",
    "variant_name = \"AllTraffic\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58a5203-3761-4b53-b492-cecd44cff541",
   "metadata": {},
   "source": [
    "From your endpoint and variant name, register a scalable target to configure overall limits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d0d2a7-4d9b-42f9-9f0c-db45d552a3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "appscaling = boto3.client(\"application-autoscaling\")\n",
    "\n",
    "# Define and register your endpoint variant\n",
    "appscaling.register_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    MinCapacity=0,  # (MinCapacity 0 not supported with real-time endpoints)\n",
    "    MaxCapacity=5,\n",
    ")\n",
    "print(f\"Endpoint registered with auto-scaling service: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd28f7a-af12-453d-8eec-8787ad47a796",
   "metadata": {},
   "source": [
    "We can also list any scaling policies that may already be active on this resource:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf1ee55-0ee7-4c87-bcb5-5e3526ae5b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "appscaling.describe_scaling_policies(ResourceId=resource_id, ServiceNamespace=\"sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfe4ceb-c4ab-4621-b02a-cb52e4cc869a",
   "metadata": {},
   "source": [
    "As discussed in the [SageMaker Asynchronous Inference Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-autoscale.html), the typical recommended scaling policy for asynchronous endpoints is to track a target on the number of queued requests per active instance - `ApproximateBacklogSizePerInstance`.\n",
    "\n",
    "However, ⚠️ setting this target value `>=1.0` can yield **un-bounded latency** for single requests arriving when the endpoint has scaled off to 0 instances - because scale-out will not be triggered until a big enough queue has formed.\n",
    "\n",
    "You can **combine multiple policies** to set up backlog target tracking but also ensure at least one instance gets started when any requests are in queue, using the alternative `HasBacklogWithoutCapacity` metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052a1c96-f81e-4001-9ff6-81f355c06356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main backlog-per-instance target tracking policy:\n",
    "scaling_policy_resp = appscaling.put_scaling_policy(\n",
    "    PolicyName=\"BacklogTargetTracking\",\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    PolicyType=\"TargetTrackingScaling\",\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        \"TargetValue\": 4.0,\n",
    "        \"CustomizedMetricSpecification\": {\n",
    "            \"MetricName\": \"ApproximateBacklogSizePerInstance\",\n",
    "            \"Namespace\": \"AWS/SageMaker\",\n",
    "            \"Dimensions\": [\n",
    "                {\"Name\": \"EndpointName\", \"Value\": endpoint_name},\n",
    "            ],\n",
    "            \"Statistic\": \"Average\",\n",
    "        },\n",
    "        \"ScaleInCooldown\": 5 * 60,  # (seconds)\n",
    "        \"ScaleOutCooldown\": 4 * 60,  # (seconds)\n",
    "    },\n",
    ")\n",
    "print(f\"Created/updated scaling policy ARN:\\n{scaling_policy_resp['PolicyARN']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f8d34a-0129-4309-a2a6-1ce581714ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra policy to ensure one-off requests get processed promptly:\n",
    "scaling_policy_resp = appscaling.put_scaling_policy(\n",
    "    PolicyName=\"BootstrapSingleRequests\",\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    PolicyType=\"StepScaling\",\n",
    "    StepScalingPolicyConfiguration={\n",
    "        \"AdjustmentType\": \"ChangeInCapacity\",\n",
    "        \"StepAdjustments\": [{\"MetricIntervalLowerBound\": 1.0, \"ScalingAdjustment\": +1}],\n",
    "        \"Cooldown\": 150,  # (Seconds)\n",
    "        \"MetricAggregationType\": \"Average\",\n",
    "    },\n",
    ")\n",
    "print(f\"Created/updated scaling policy ARN:\\n{scaling_policy_resp['PolicyARN']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09177701-224c-4e49-bfcd-b14310a95d9d",
   "metadata": {},
   "source": [
    "Your endpoint should now be set up to auto-scale. Refer to the [Endpoints section of the SageMaker Console](https://console.aws.amazon.com/sagemaker/home?#/endpoints) on the detail page for your target endpoint to check.\n",
    "\n",
    "### Disabling auto-scaling\n",
    "\n",
    "If you'd like to de-register an endpoint from auto-scaling, you can delete attached policies and de-register the target as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd557f2-e70c-410d-b9c6-715744b95dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = appscaling.describe_scaling_policies(\n",
    "    ResourceId=resource_id,\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    ")[\"ScalingPolicies\"]\n",
    "\n",
    "print(f\"Deleting scaling policies for {resource_id}:\")\n",
    "time.sleep(3)\n",
    "\n",
    "for policy in policies:\n",
    "    appscaling.delete_scaling_policy(\n",
    "        PolicyName=policy[\"PolicyName\"],\n",
    "        ServiceNamespace=policy[\"ServiceNamespace\"],\n",
    "        ResourceId=policy[\"ResourceId\"],\n",
    "        ScalableDimension=policy[\"ScalableDimension\"],\n",
    "    )\n",
    "    print(f\" - {policy['PolicyName']}\")\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c4922b-c218-4569-94f0-57b10b3c94bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"De-registering from auto-scaling:\\n  {resource_id}\")\n",
    "time.sleep(3)\n",
    "\n",
    "appscaling.deregister_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    ")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c980dcd-8263-4d5e-bbcc-0eeaf43e022f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*[Back to contents](#Contents)*"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
