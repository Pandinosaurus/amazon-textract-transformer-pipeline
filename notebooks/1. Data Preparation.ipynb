{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Processing Amazon Textract with Location-Aware Transformers\n",
    "\n",
    "# Part 1: Introduction and Data Collection\n",
    "\n",
    "> *This notebook works well with the `Python 3 (Data Science)` kernel on SageMaker Studio*\n",
    "\n",
    "[**LayoutLM** (2019, Xu et al)](https://arxiv.org/abs/1912.13318) is a BERT-like language model architecture in which the *position embedding* inputs (usually encoding the position of each word/token in the input sequence, as detailed in [this paper](https://openreview.net/forum?id=onxoVA9FxMw)) are modified to encode the **absolute position of the word/token on a page**.\n",
    "\n",
    "Architectures like this enable us to build ML models which are aware of both *text content* and *page position*: especially useful for analyzing and post-processing OCR results from services like [Amazon Textract](https://aws.amazon.com/textract/), which returns both the detected text and the geometry of each word in the input document.\n",
    "\n",
    "Since LayoutLM is based on a standard multi-task text transformer architecture with customizations to the input processing layer, this approach could be generalized to a wide range of task types using both text and position information, like:\n",
    "\n",
    "- \"Self-supervised\" pre-training on Textracted but otherwise unlabelled documents\n",
    "- Document/page/sequence classification\n",
    "- Entity extraction (token/word classification)\n",
    "- Span extraction and extractive question answering\n",
    "- \"Translation\", generative question answering or other sequence generation\n",
    "\n",
    "Some of these use cases (notably pre-training, sequence classification and token/word classification) are already supported in the LayoutLM [implementation](https://huggingface.co/transformers/model_doc/layoutlm.html) provided in the popular open source [Hugging Face Transformers library](https://huggingface.co/transformers/model_doc/layoutlm.html).\n",
    "\n",
    "In this sample we'll review an example use case where Amazon Textract's [built-in functionality](https://aws.amazon.com/textract/features/) for extracting key-value \"Forms\" data and structured \"Tables\" data helps with some examples... But misses others due to the complexity of the document.\n",
    "\n",
    "This first notebook will focus on preparing and annotating data, before we move on to training, deploying, and integrating models in later notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started: Dependencies and configuration\n",
    "\n",
    "First there are some additional libraries we need to install that aren't present in the SageMaker kernel environments by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tool for building customized container images and pushing to Amazon ECR:\n",
    "!pip install sagemaker-studio-image-build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Helper for interpreting Textract results:\n",
    "!pip install amazon-textract-response-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Notebook 2 requires sagemaker>=2.49 for Hugging Face container versions:\n",
    "!pip install \"sagemaker>=2.49,<3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the requried libraries installed, we're ready to import dependencies and set up some basic configuration including which [Amazon S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingBucket.html) and folder/prefix data will be uploaded to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python Built-Ins:\n",
    "import json\n",
    "from logging import getLogger\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3  # AWS SDK for Python\n",
    "from IPython.display import HTML  # To display rich content in notebook\n",
    "import pandas as pd  # For tabular data analysis\n",
    "import sagemaker  # High-level SDK for SageMaker\n",
    "from tqdm.notebook import tqdm  # Progress bars\n",
    "\n",
    "# Local Dependencies:\n",
    "import util\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "bucket_name = sagemaker.Session().default_bucket()\n",
    "bucket_prefix = \"textract-transformers/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some sections below we'll need to reference resources created by the *[AWS CloudFormation](https://aws.amazon.com/cloudformation/) solution stack* you spun up earlier. If you didn't do this step yet, refer to the [README.md](../README.md) in the top level of the repository for instructions.\n",
    "\n",
    "The solution stack stores these useful variables in [AWS Systems Manager Parameter Store](https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html) and we use the `util.project` utility module used below to fetch them. This is a transferable pattern you can use to connect from data science notebooks to deployed ML project resources in the cloud by project name/ID.\n",
    "\n",
    "▶️ **Check** in the [CloudFormation Console](https://console.aws.amazon.com/cloudformation/home?#/stacks) that the `ProjectId` parameter for your OCR Pipeline Stack matches the default `ocr-transformers-demo` value below: Otherwise change the code below to match.\n",
    "\n",
    "> ⚠️ If you get an **AccessDeniedException** (ClientError) below, it's likely your [SageMaker execution role](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) doesn't have the required `ssm:GetParameters` permission to look up the OCR pipeline stack parameters.\n",
    ">\n",
    "> To fix this, you can click your execution role in the [IAM Roles Console](https://console.aws.amazon.com/iamv2/home#/roles) and use the **Attach policies** button to attach the `PipelineDataSciencePolicy` created by the stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = util.project.init(\"ocr-transformers-demo\")\n",
    "    print(config)\n",
    "except Exception as e:\n",
    "    try:\n",
    "        print(f\"Your SageMaker execution role is: {sagemaker.get_execution_role()}\")\n",
    "    except:\n",
    "        print(\"Couldn't look up your SageMaker execution role\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the example data\n",
    "\n",
    "For our example, we'll be exploring (a recent quarter's snapshot of) the [Credit card agreements database](https://www.consumerfinance.gov/credit-cards/agreements/) published by the United States' [Consumer Finance Protection Bureau](https://www.consumerfinance.gov/).\n",
    "\n",
    "This dataset includes specimen credit card agreement documents from providers across the US, and is interesting for our purposes because the documents are:\n",
    "\n",
    "- **Diverse** in formatting, as various providers present the required information in different ways\n",
    "- **Representative of commercial** documents - rather than, for example, academic papers which might have quite different tone and structure\n",
    "- **Complex** in structure, with common data points in theory (e.g. interest rates, fees, etc) - but a lot of nuances and differences between documents in practice.\n",
    "\n",
    "Below, we download a recent (approx. 750MB) archive from the dataset and extract the files (approx. 900MB uncompressed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "!wget -O data/CC_Agreements.zip https://files.consumerfinance.gov/a/assets/Credit_Card_Agreements_2020_Q4.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_file_types = { \"jpeg\", \"jpg\", \"pdf\", \"png\" }\n",
    "\n",
    "# Extract the zip:\n",
    "with ZipFile(\"data/CC_Agreements.zip\", \"r\") as fzip:\n",
    "    rel_filepaths_all = sorted([\n",
    "        f.filename\n",
    "        for f in fzip.infolist()\n",
    "        if not (f.is_dir() or \"__MACOSX\" in f.filename)\n",
    "    ])\n",
    "    print(f\"Found {len(rel_filepaths_all)} files in archive\")\n",
    "    print(\"Extracting...\")\n",
    "    fzip.extractall(\"data/raw\")\n",
    "\n",
    "rel_filepaths = sorted(\n",
    "    [f for f in rel_filepaths_all if f.lower().rpartition(\".\")[2] in valid_file_types]\n",
    ")\n",
    "\n",
    "# Clean up unneeded files and remap if the folder became nested:\n",
    "original_root_items = os.listdir(\"data/raw\")\n",
    "if \"__MACOSX\" in original_root_items:\n",
    "    shutil.rmtree(\"data/raw/__MACOSX\")\n",
    "if len(original_root_items) < 5:\n",
    "    try:\n",
    "        folder = next(f for f in original_root_items if f.startswith(\"Credit_Card_Agreements\"))\n",
    "        print(f\"De-nesting folder '{folder}'...\")\n",
    "        for sub in os.listdir(f\"data/raw/{folder}\"):\n",
    "            shutil.move(f\"data/raw/{folder}/{sub}\", f\"data/raw/{sub}\")\n",
    "        rel_filepaths = [\n",
    "            f[len(folder + \"/\"):] if f.startswith(folder + \"/\") else f for f in rel_filepaths\n",
    "        ]\n",
    "    except StopIteration:\n",
    "        pass\n",
    "\n",
    "print(f\"Found {len(rel_filepaths)} valid files for OCR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can explore these documents in the `data/raw` folder from the file browser - or even pull them through to display inline here in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\n",
    "    '<iframe src=\"{}\" width=100% height=600 type=\"application/pdf\"></iframe>'.format(\n",
    "        # Edit the below (e.g. 0, 1, 2) to see different documents:\n",
    "        \"data/raw/\" + rel_filepaths[0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multi-page documents like these PDFs, Amazon Textract [requires us](https://docs.aws.amazon.com/textract/latest/dg/async.html) to use the async APIs and pre-load the documents to S3.\n",
    "\n",
    "Therefore we'll upload the PDFs to use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Uploading raw PDFs to {raw_s3uri}...\")\n",
    "!aws s3 sync --quiet data/raw $raw_s3uri\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the challenge\n",
    "\n",
    "We have our source documents, so what will we try to extract about them?\n",
    "\n",
    "There are many ways position-aware NLP models might be applied to OCR outputs: For example to generate structured summaries, provide translations, answer questions, or just classify the documents.\n",
    "\n",
    "A common requirement in document analytics and process automation though, is to extract particular **'fields' of interest**: Known attributes expected to be present in all/most of the documents, which would be interesting to compare between them.\n",
    "\n",
    "In this example we'll tackle this as an **entity detection** task **via word classification**:\n",
    "\n",
    "- Defining a list of field/entity types of interest\n",
    "- Classifying each `WORD` in the document to these types, using a Hugging Face [LayoutLMForTokenClassification](https://huggingface.co/transformers/model_doc/layoutlm.html#layoutlmfortokenclassification) model\n",
    "- ...And finally grouping individual words together (via simple rule-based post-processing) to detect the entities/fields\n",
    "\n",
    "Some **benefits** of this approach are:\n",
    "\n",
    "- Results are traceable all the way back to the detected word blocks from the OCR engine; rather than with a text generation method where the output of the model may not correspond 1:1 with detected word inputs.\n",
    "- Annotation effort is relatively minimal; since we only need to highlight the documents, rather than typing out custom corrections, answers, etc.\n",
    "\n",
    "Some **drawbacks** are:\n",
    "\n",
    "- Since it only tags detected words, this model will not be able to *intelligently correct OCR errors* or *standardize form* (e.g. of dates) like a text generation method could learn to.\n",
    "- Since the ML component only extends to word classification, we're still relying on (usually good, helped by Amazon Textract) rule-based heuristics to group same-type words together to detect multi-word entities.\n",
    "\n",
    "Below, we'll define the set of fields/entities to be detected and their configuration aspects:\n",
    "\n",
    "> ⚠️ **Warning:** Although you may **edit** the configuration below, you'll no longer be able to use the pre-annotated data sample we provide in `data/annotations` to accelerate model training (unless the classes are still defined in the same order, and labelled in a consistent way with the previous guidelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.postproc.config import FieldConfiguration\n",
    "\n",
    "# For config API details, you can see the docs in the source file or run:\n",
    "# help(FieldConfiguration)\n",
    "\n",
    "fields = [\n",
    "    # (To prevent human error, enter class_id=0 each time and update programmatically below)\n",
    "    FieldConfiguration(0, \"Agreement Effective Date\", optional=True, select=\"first\",\n",
    "        annotation_guidance=(\n",
    "            \"<p>Avoid labeling extraneous dates which are not necessarily the effective date of \"\n",
    "            \"the document: E.g. copyright dates/years, or other dates mentioned in text.</p> \"\n",
    "            \"<p>Do not include unnecessary qualifiers e.g. 'from 2020/01/01'.</p>\"\n",
    "        ),\n",
    "    ),\n",
    "    FieldConfiguration(0, \"APR - Introductory\", optional=True, select=\"confidence\",\n",
    "        annotation_guidance=(\n",
    "            \"<p>Use this class (instead of the others) for <em>ANY</em> case where the rate is \"\n",
    "            \"offered for a fixed introductory period - regardless of interest rate subtype e.g. \"\n",
    "            \"balance transfers, purchases, etc.</p> \"\n",
    "            \"<p>Include the term of the introductory period in cases where it's directly listed \"\n",
    "            \"(e.g. '20.00% for the first 6 months'). Try to minimize/exclude extraneous \"\n",
    "            \"information about the offer (e.g. '20.00% for the first 6 months after account \"\n",
    "            \"opening').</p> \"\n",
    "            \"<p>'Prime rate + X%' mentions are acceptable and should be labeled.</p>\"\n",
    "        ),\n",
    "    ),\n",
    "    FieldConfiguration(0, \"APR - Balance Transfers\", optional=True, select=\"confidence\",\n",
    "        annotation_guidance=(\n",
    "            \"<p>Use for interest rates which are specific to balance transfers.</p> \"\n",
    "            \"<p>Avoid including extraneous information about the terms of balance transfers, or \"\n",
    "            \"using for fixed-term introductory rates.</p> \"\n",
    "            \"<p>'Prime rate + X%' mentions are acceptable and should be labeled.</p>\"\n",
    "        ),\n",
    "    ),\n",
    "    FieldConfiguration(0, \"APR - Cash Advances\", optional=True, select=\"confidence\",\n",
    "        annotation_guidance=(\n",
    "            \"<p>Use for interest rates which are specific to cash advances.</p> \"\n",
    "            \"<p>Avoid including extraneous information about the terms of cash advances, or using \"\n",
    "            \"for fixed-term introductory rates.</p> \"\n",
    "            \"<p>'Prime rate + X%' mentions are acceptable and should be labeled.</p>\"\n",
    "        ),\n",
    "    ),\n",
    "    FieldConfiguration(0, \"APR - Purchases\", optional=True, select=\"confidence\",\n",
    "        annotation_guidance=(\n",
    "            \"<p>Use for interest rates which are specific to purchases.</p> \"\n",
    "            \"<p>'Prime rate + X%' mentions are acceptable and should be labeled.</p>\"\n",
    "        ),\n",
    "    ),\n",
    "    FieldConfiguration(0, \"APR - Penalty\", optional=True, select=\"confidence\",\n",
    "        annotation_guidance=(\n",
    "            \"<p>Use for penalty interest rates applied under certain conditions.</p> \"\n",
    "            \"<p><em>Exclude</em> include information about the conditions under which the penalty \"\n",
    "            \"rate comes into effect: Only include the interest rate which will be applied.</p> \"\n",
    "            \"<p>'Prime rate + X%' mentions are acceptable and should be labeled.</p>\"\n",
    "        ),\n",
    "    ),\n",
    "    FieldConfiguration(0, \"APR - General\", optional=True, select=\"confidence\",\n",
    "        annotation_guidance=(\n",
    "            \"<p>Use for interest rates which are general and not specifically tied to a \"\n",
    "            \"particular transaction type e.g. purchases / balance transfers.</p> \"\n",
    "            \"<p>Avoid using for fixed-term introductory rates.</p> \"\n",
    "            \"<p>'Prime rate + X%' mentions are acceptable and should be labeled.</p>\"\n",
    "        ),\n",
    "    ),\n",
    "    FieldConfiguration(0, \"APR - Other\", optional=True, select=\"confidence\",\n",
    "        # TODO: Remove this class\n",
    "        annotation_guidance=(\n",
    "            \"<p>Use only for interest rates which don't fall in to any other category (including \"\n",
    "            \"general or introductory rates). You may not see any examples in the data.</p> \"\n",
    "            \"<p>Avoid using for fixed-term introductory rates.</p> \"\n",
    "            \"<p>'Prime rate + X%' mentions are acceptable and should be labeled.</p>\"\n",
    "        ),\n",
    "    ),\n",
    "    FieldConfiguration(0, \"Fee - Annual\", optional=True, select=\"confidence\",\n",
    "        annotation_guidance=(\n",
    "            \"<p>Include cases where the document explicitly indicates no fee e.g. 'None'</p> \"\n",
    "            \"<p>Avoid any introductory terms e.g. '$0 for the first 6 months' or extraneous \"\n",
    "            \"words: Label only the standard fee.</p> \"\n",
    "            \"<p>Label only the annual amount of the fee, in cases where other breakdowns are \"\n",
    "            \"specified: E.g. '$120', not '$10 per month ($120 per year)'.</p> \"\n",
    "        ),\n",
    "    ),\n",
    "    FieldConfiguration(0, \"Fee - Balance Transfer\", optional=True, select=\"confidence\",\n",
    "        annotation_guidance=(\n",
    "            # TODO: Review\n",
    "            \"<p>Try to be concise and exclude extra terms where not necessary</p>\"\n",
    "        ),\n",
    "    ),\n",
    "    FieldConfiguration(0, \"Fee - Late Payment\", optional=True, select=\"confidence\",\n",
    "        annotation_guidance=(\n",
    "            \"<p>Label only the fee, not the circumstances in which it is payable.</p> \"\n",
    "            \"<p>Limits e.g. 'Up to $25' are acceptable (don't just label '$25').</p> \"\n",
    "            \"<p>Do <em>NOT</em> include non-specific mentions of pass-throgh costs (e.g. 'legal \"\n",
    "            \"costs', 'reasonable expenses', etc.) incurred in the general collections process.</p>\"\n",
    "        ),\n",
    "    ),\n",
    "    FieldConfiguration(0, \"Fee - Returned Payment\", optional=True, select=\"confidence\",\n",
    "        annotation_guidance=(\n",
    "            \"<p>Label only the fee, not the circumstances in which it is payable.</p> \"\n",
    "            \"<p>Limits e.g. 'Up to $25' are acceptable (don't just label '$25').</p>\"\n",
    "        ),\n",
    "    ),\n",
    "    FieldConfiguration(0, \"Fee - Foreign Transaction\", optional=True, select=\"shortest\",\n",
    "        annotation_guidance=(\n",
    "            \"<p>Do <em>NOT</em> include explanations of how exchange rates are calculated or \"\n",
    "            \"non-specific indications of margins between rates. <em>DO</em> include specific \"\n",
    "            \"charges/margins with <em>brief</em> clarifying info where listed e.g. '3% of the US \"\n",
    "            \"dollar amount'.</p>\"\n",
    "        ),\n",
    "    ),\n",
    "    FieldConfiguration(0, \"Fee - Other\", ignore=True,\n",
    "        annotation_guidance=(\n",
    "            \"<p>Common examples include: Minimum interest charge, cash advance fees, and \"\n",
    "            \"overlimit fees.</p> \"\n",
    "            \"<p>Do <em>NOT</em> include fixed-term introductory rates for fees (e.g. '$0 during \"\n",
    "            \"the first year. After the first year...') - only the standard fees</p> \"\n",
    "            \"<p><em>DO</em> include qualifying information on the amount and limits of the fee, \"\n",
    "            \"e.g. '$5 or 5% of the amount of each transaction, whichever is the greater'.</p> \"\n",
    "            \"<p>Do <em>NOT</em> include general information on the nature of the fee and \"\n",
    "            \"circumstances under which it is applied: E.g. 'Cash advance fee' or 'If the amount \"\n",
    "            \"of interest payable is...'</p>\"\n",
    "        ),\n",
    "    ),\n",
    "    FieldConfiguration(0, \"Card Name\",\n",
    "        annotation_guidance=(\n",
    "            \"<p>Label instances of the brand name of specific card(s) offered by the provider \"\n",
    "            \"under the agreement, e.g. 'Rewards Platinum Card'</p> \"\n",
    "            \"<p>Include the ' Card' suffix where available, but also annotate instances without \"\n",
    "            \"such as 'Rewards Platinum'</p> \"\n",
    "            \"<p><em>Avoid</em> including the Provider Name (use the separate class for this) e.g. \"\n",
    "            \"'AnyCompany Rewards Card' unless it's been substantially modified/abbreviated for \"\n",
    "            \"the card name (e.g. 'AnyCo Rewards Card') or the company name is different from the \"\n",
    "            \"Credit card provider (e.g. AnyBank offering a store credit card for AnyCompany)</p> \"\n",
    "            \"<p>Do <em>NOT</em> include fixed-term introductory rates for fees (e.g. '$0 during \"\n",
    "            \"the first year. After the first year...') - only the standard fees</p> \"\n",
    "            \"<p><em>Avoid</em> labeling generic payment provider names e.g. 'VISA card' or \"\n",
    "            \"'Mastercard', except in contexts where the provider clearly uses them as the brand \"\n",
    "            \"name for the offered card (e.g. 'VISA Card' from 'AnyCompany VISA Card'.</p>\"\n",
    "        ),\n",
    "    ),\n",
    "    FieldConfiguration(0, \"Provider Address\", optional=True, select=\"confidence\",\n",
    "        annotation_guidance=(\n",
    "            \"<p>Include department or 'attn:' lines where present (but not Provider Name where \"\n",
    "            \"used at the start of an address e.g. 'AnyCompany; 100 Main Street...').</p> \"\n",
    "            \"<p>Include zip/postcode where present.</p> \"\n",
    "            \"<p><em>Avoid</em> labeling addresses for non-provider entities, such as watchdogs, \"\n",
    "            \"market regulators, or independent agencies.</p>\"\n",
    "        ),\n",
    "    ),\n",
    "    FieldConfiguration(0, \"Provider Name\", select=\"longest\",\n",
    "        annotation_guidance=(\n",
    "            \"<p>Label the name of the card provider: Including abbreviated mentions.</p>\"\n",
    "        ),\n",
    "    ),\n",
    "    FieldConfiguration(0, \"Min Payment Calculation\", ignore=True,\n",
    "        annotation_guidance=(\n",
    "            \"<p>Label clauses describing how the minimum payment is calculated.</p> \"\n",
    "            \"<p>Exclude lead-in e.g. 'The minimum payment is calculated as...' and label directly \"\n",
    "            \"from e.g. 'the minimum of...'.</p> \"\n",
    "            \"<p>Do <em>NOT</em> include clauses from related subjects e.g. how account balance is \"\n",
    "            \"calculated</p>\"\n",
    "        ),\n",
    "    ),\n",
    "    FieldConfiguration(0, \"Local Terms\", ignore=True,\n",
    "        annotation_guidance=(\n",
    "            \"<p>Label full terms specific to residents of certain states/countries, or applying \"\n",
    "            \"only in particular jurisdictions.</p> \"\n",
    "            \"<p><em>Include</em> the scope of where the terms apply e.g. 'Residents of GA and \"\n",
    "            \"VA...'</p> \"\n",
    "            \"<p><em>Include</em> locally-applicable interest rates, instead of annotating these \"\n",
    "            \"with the 'APR - ' classes</p>\"\n",
    "        ),\n",
    "    )\n",
    "]\n",
    "for ix, cfg in enumerate(fields):\n",
    "    cfg.class_id = ix\n",
    "\n",
    "# Save the configuration to file:\n",
    "with open(\"data/field-config.json\", \"w\") as f:\n",
    "    f.write(json.dumps(\n",
    "        [cfg.to_dict() for cfg in fields],\n",
    "        indent=2,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_classes = [f.name for f in fields]\n",
    "print(\"\\n\".join(entity_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "\n",
    "To efficiently annotate training data for entity extraction on documents, we'll want to work **visually**: Highlighting matches, and perhaps also collecting manual transcription reviews - in case we'd like to extend the model later to support correcting text.\n",
    "\n",
    "[Amazon SageMaker Ground Truth](https://aws.amazon.com/sagemaker/groundtruth/) provides an out-of-the-box annotation UI [for bounding boxes](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-bounding-box.html) which will be useful for this: And can also be incorporated within [customized annotation UIs](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-custom-templates.html) via the [crowd-bounding-box](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-ui-template-crowd-bounding-box.html) element.\n",
    "\n",
    "However, at the time of writing, the bounding box annotation tool supports images but not PDFs.\n",
    "\n",
    "Therefore to prepare for data annotation we'll need to:\n",
    "\n",
    "- Run our documents through Amazon Textract\n",
    "- Extract individual page images from the PDFs to use through the annotation UI\n",
    "- Collate the page images and Textract results together, ready for annotation\n",
    "\n",
    "For a significantly sized corpus like this, we'd also benefit from filtering down the data a little - to save time and cost by Textracting and converting only the amount of data we'll need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter a sample of the document set\n",
    "\n",
    "To limit down our input corpus, we'll:\n",
    "\n",
    "- Apply some basic filename-based rules to try and exclude the few Spanish-language documents in the corpus, since the pre-trained model we'll use later is for English only\n",
    "- Take a random (but reproducible) split of the first N documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DOCS_KEPT = 120\n",
    "\n",
    "def include_filename(name: str) -> bool:\n",
    "    \"\"\"Filter out docs whose filenames suggest they're likely Spanish/non-English\"\"\"\n",
    "    if not name:\n",
    "        return name\n",
    "    name_l = name.lower()\n",
    "    if \"spanish\" in name_l:\n",
    "        return False\n",
    "    if re.search(r\"espa[nñ]ol\", name_l):\n",
    "        return False\n",
    "    if \"tarjeta\" in name_l or re.search(r\"cr[eé]dito\", name_l):\n",
    "        return False\n",
    "    if re.search(r\"[\\[\\(]esp?[\\]\\)]\", name_l):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_preannotated_filepaths(exclude_job_names=[]) -> list:\n",
    "    \"\"\"List out (alphabetically) the relative filepaths for which annotations are already exist\"\"\"\n",
    "    filepaths = set()  # Protect against introducing duplicates\n",
    "    for job_folder in os.listdir(\"data/annotations\"):\n",
    "        if job_folder in exclude_job_names:\n",
    "            logger.info(f\"Skipping excluded job {job_folder}\")\n",
    "            continue\n",
    "        manifest_file = os.path.join(\n",
    "            \"data\",\n",
    "            \"annotations\",\n",
    "            job_folder,\n",
    "            \"manifests\",\n",
    "            \"output\",\n",
    "            \"output.manifest\",\n",
    "        )\n",
    "        if not os.path.isfile(manifest_file):\n",
    "            if os.path.isdir(os.path.join(\"data\", \"annotations\", job_folder)):\n",
    "                logger.warning(f\"Skipping job {job_folder}: No output manifest at {manifest_file}\")\n",
    "            continue\n",
    "        with open(manifest_file, \"r\") as f:\n",
    "            textract_s3keys = [\n",
    "                json.loads(l)[\"textract-ref\"][len(\"s3://\"):].partition(\"/\")[2] for l in f\n",
    "            ]\n",
    "            # S3 keys are like some/prefix/data/textracted/subfolders/file.pdf/consolidated.json\n",
    "            # We want subfolders/file.pdf\n",
    "            filepaths.update([\n",
    "                k.partition(\"data/textracted/\")[2].rpartition(\"/\")[0] for k in textract_s3keys\n",
    "            ])\n",
    "    return sorted(filepaths)\n",
    "\n",
    "\n",
    "preannotated_filepaths = get_preannotated_filepaths()\n",
    "if N_DOCS_KEPT < len(preannotated_filepaths):\n",
    "    raise ValueError(\n",
    "        \"Existing annotations cannot be used for model training unless the target documents are \"\n",
    "        \"Textracted. To proceed with fewer docs than have already been annotated, you'll need to \"\n",
    "        \"`exclude_job_names` per the 'data/annotations' folder (e.g. ['augmentation-1']) AND \"\n",
    "        \"remember to not include them in notebook 2 (model training). Alternatively, increase \"\n",
    "        f\"your N_DOCS_KEPT. (Got {N_DOCS_KEPT} vs {len(preannotated_filepaths)} prev annotations).\"\n",
    "    )\n",
    "\n",
    "# Forcibly including the pre-annotated docs *after* the shuffling ensures that the order of\n",
    "# sampling new docs is independent of what/how many have been pre-annotated:\n",
    "rel_filepaths_sample = list(filter(include_filename, rel_filepaths))\n",
    "random.Random(1337).shuffle(rel_filepaths_sample)\n",
    "rel_filepaths_sample = [f for f in rel_filepaths_sample if f not in preannotated_filepaths]\n",
    "rel_filepaths_sample = sorted(\n",
    "    preannotated_filepaths\n",
    "    + rel_filepaths_sample[:N_DOCS_KEPT - len(preannotated_filepaths)]\n",
    ")\n",
    "\n",
    "print(f\"Extracted random sample of {len(rel_filepaths_sample)} docs\")\n",
    "rel_filepaths_sample[:5] + [\"...\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract clean input images\n",
    "\n",
    "To annotate our documents with SageMaker Ground Truth image task UIs, we need **individual page images**, stripped of EXIF rotation metadata (because, at the time of writing, SMGT ignores this rotation for annotation consistency) and converted to compatible formats (since some browsers cannot render certain formats - such as TIFF).\n",
    "\n",
    "For large corpora this process of splitting PDFs and rotating and converting images may require significant resources, but is easy to parallelize.\n",
    "\n",
    "Therefore instead of pre-processing the raw documents here in the notebook, this is a good use case for a scalable [SageMaker Processing Job](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html).\n",
    "\n",
    "First, our processing job will require a base container image to work with. The PDF reading tools we use aren't installed by default in pre-built SageMaker containers and aren't `pip install`able, so below we use the [SageMaker Studio Image Build CLI](https://github.com/aws-samples/sagemaker-studio-image-build-cli) to build a customized image based on the SageMaker Scikit-Learn container and upload it to the [Amazon Elastic Container Registry (ECR)](https://aws.amazon.com/ecr/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ecr_repo_name = \"sm-scikit-ocrtools\"\n",
    "ecr_image_tag = \"pytorch-1.7-cpu\"\n",
    "\n",
    "base_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=os.environ[\"AWS_REGION\"],\n",
    "    instance_type=\"ml.c5.xlarge\",  # (Just used to check whether GPUs/accelerators are used)\n",
    "    py_version=\"py3\",\n",
    "    image_scope=\"training\",\n",
    "    version=\"1.7\",\n",
    ")\n",
    "\n",
    "!sm-docker build ./preproc \\\n",
    "    --repository {ecr_repo_name}:{ecr_image_tag} \\\n",
    "    --role {config.sm_image_build_role} \\\n",
    "    --build-arg BASE_IMAGE={base_image_uri}\n",
    "\n",
    "# Since the above is a shell command, we'll need to reconstruct the built URI here in Python too:\n",
    "account_id = sagemaker.Session().account_id()\n",
    "region = os.environ[\"AWS_REGION\"]\n",
    "ecr_image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{ecr_repo_name}:{ecr_image_tag}\"\n",
    "\n",
    "# Check from Python the image was successfully created:\n",
    "ecr = boto3.client(\"ecr\")\n",
    "imgs_desc = ecr.describe_images(\n",
    "    registryId=account_id,\n",
    "    repositoryName=ecr_repo_name,\n",
    "    imageIds=[{ \"imageTag\": ecr_image_tag }],\n",
    ")\n",
    "assert len(imgs_desc[\"imageDetails\"]) > 0, f\"Couldn't find ECR image {ecr_image_uri} after build\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define the inputs for the processing job.\n",
    "\n",
    "To process the whole `data/raw` corpus, you could simply pass the whole `data/raw` prefix in S3 as input to the job (As shown in the commented-out *Option 2* below) and scale up the `instance_count` to complete the work quickly.\n",
    "\n",
    "To process just a sample subset of files for speed in our demo, we'll create a **manifest file** listing just the documents we want.\n",
    "\n",
    "> ⚠️ **Note:** 'Non-augmented' manifest files are still JSON-based, but a different format from the other dataset manifests we'll be using through this sample. You can find guidance for manifests as used here on the [S3DataSource API doc](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_S3DataSource.html), and separate information on the \"augmented\" manifests as used later with SageMaker Ground Truth in the [Ground Truth documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-input-data-input-manifest.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "\n",
    "imgs_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/imgs-clean\"\n",
    "\n",
    "#### OPTION 1: For processing the rel_filepaths_sample subset of raw docs only:\n",
    "# Prepare a manifest file:\n",
    "os.makedirs(\"data/preproc\", exist_ok=True)\n",
    "preproc_input_manifest_path = \"data/preproc/dataclean-input.manifest.json\"\n",
    "with open(preproc_input_manifest_path, \"w\") as f:\n",
    "    f.write(json.dumps(\n",
    "        [{ \"prefix\": raw_s3uri + \"/\" }]\n",
    "        + rel_filepaths_sample\n",
    "    ))\n",
    "\n",
    "# Upload the manifest to S3:\n",
    "preproc_input_manifest_s3uri = f\"s3://{bucket_name}/{bucket_prefix}{preproc_input_manifest_path}\"\n",
    "!aws s3 cp {preproc_input_manifest_path} {preproc_input_manifest_s3uri}\n",
    "\n",
    "# Set the processing job inputs to reference the manifest:\n",
    "preproc_inputs = [\n",
    "    ProcessingInput(\n",
    "        destination=\"/opt/ml/processing/input/raw\",  # Expected input location, per our script\n",
    "        input_name=\"raw\",\n",
    "        s3_data_distribution_type=\"ShardedByS3Key\",  # Distribute between instances, if multiple\n",
    "        s3_data_type=\"ManifestFile\",\n",
    "        source=preproc_input_manifest_s3uri,  # Manifest of sample raw documents\n",
    "    ),\n",
    "]\n",
    "print(\"Selected sample subset of documents\")\n",
    "#### END OPTION 1\n",
    "\n",
    "#### OPTION 2: For processing the whole data/raw folder:\n",
    "# preproc_inputs = [\n",
    "#     ProcessingInput(\n",
    "#         destination=\"/opt/ml/processing/input/raw\",  # Expected input location, per our script\n",
    "#         input_name=\"raw\",\n",
    "#         s3_data_distribution_type=\"ShardedByS3Key\",  # Distribute between instances, if multiple\n",
    "#         source=raw_s3uri,  # S3 prefix for full raw document collection\n",
    "#     ),\n",
    "# ]\n",
    "# print(\"Selected whole corpus\")\n",
    "#### END OPTION 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script we'll be using to process the documents is in the same folder as the Dockerfile used earlier to build the container image: [preproc/imgclean.py](preproc/imageclean.py).\n",
    "\n",
    "The code parallelizes processing across available CPUs, and the `ShardedByS3Key` setting used on our [ProcessingInput](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.ProcessingInput) above distributes documents between instances if muliple are provided - so you should be able to `instance_type` and `instance_count` of the job if needed to take advantage of what resources you have available. The process is typically CPU-bound, so the `ml.c*` families are likely a good fit.\n",
    "\n",
    "> ⏰ **Note:** In our tests, it took (including job start-up overheads):\n",
    ">\n",
    "> - About 9 minutes to process the 120-document sample with 2x `ml.c5.xlarge` instances\n",
    "> - About 17 minutes to process the full 2,541-document corpus with 5x `ml.c5.4xlarge` instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor = ScriptProcessor(\n",
    "    base_job_name=\"ocr-img-dataclean\",\n",
    "    command=[\"python3\"],\n",
    "    image_uri=ecr_image_uri,  # As created above\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    max_runtime_in_seconds=60*60,\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    volume_size_in_gb=15,\n",
    ")\n",
    "\n",
    "processor.run(\n",
    "    code=\"preproc/imgclean.py\",  # PDF splitting / image conversion script\n",
    "    inputs=preproc_inputs,  # Either whole corpus or sample, as above\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            destination=imgs_s3uri,\n",
    "            output_name=\"imgs-clean\",\n",
    "            s3_upload_mode=\"Continuous\",\n",
    "            source=\"/opt/ml/processing/output/imgs-clean\",  # Output folder, per our script\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the images have been extracted, we'll also download them locally to the notebook for use in visualizations later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Downloading cleaned images from {imgs_s3uri}...\")\n",
    "!aws s3 sync --quiet {imgs_s3uri} data/imgs-clean\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textract the input documents\n",
    "\n",
    "Since we need to be mindful of the Amazon Textract service [quotas](https://docs.aws.amazon.com/general/latest/gr/textract.html#limits_textract) when processing large batches of documents, and the OCR pipeline solution stack is already set up - we'll use just the **OCR/Textract portion of the pipeline** to run our documents through Textract in bulk.\n",
    "\n",
    "> ⏰ **Note:** The code below may take ~6 minutes to run against a 120-document sample set and may encounter occasional rate-limiting errors. **If you see errors in the output, try re-running the cell**: Successfully processed files will be skipped in repeat runs.\n",
    "\n",
    "> ⚠️ **Note:** Refer to the [Amazon Textract Pricing Page](https://aws.amazon.com/textract/pricing/) for up-to-date guidance before running large extraction jobs.\n",
    ">\n",
    "> At the time of writing, the projected cost (in `us-east-1`, ignoring free tier allowances) of analyzing 100 documents with 10 pages on average was approximately \\\\$67 with `TABLES` and `FORMS` enabled, or \\\\$2 without. Across the full corpus, we measured the average number of pages per document at approximately 6.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "textract_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/textracted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "textract_results = util.preproc.call_textract(\n",
    "    textract_sfn_arn=config.plain_textract_sfn_arn,\n",
    "    input_base_s3uri=raw_s3uri,\n",
    "    # Can instead set rel_filepaths, to process the whole dataset (see cost note above):\n",
    "    input_relpaths=rel_filepaths_sample,\n",
    "    # You can un-comment the below `features` line to turn on both TABLES and FORMS extraction, but\n",
    "    # note that this could have a significant impact on API costs:\n",
    "    features=[],\n",
    "    output_base_s3uri=textract_s3uri,\n",
    "    skip_existing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textract_s3uris = list(filter(lambda s: isinstance(s, str), textract_results))\n",
    "print(f\"{len(textract_s3uris)} of {len(textract_results)} docs textracted successfully\")\n",
    "\n",
    "if len(textract_s3uris) < len(textract_results):\n",
    "    raise ValueError(\n",
    "        \"Are you sure you want to continue? Consider re-trying to process the failed docs\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate OCR and image data for annotation\n",
    "\n",
    "Now we have a filtered corpus of documents with Amazon Textract results, plus cleaned and standardized images for each page - all available on Amazon S3.\n",
    "\n",
    "To prepare for data annotation and later model training, we'll need to collate these together with a [manifest file](https://docs.aws.amazon.com/sagemaker/latest/dg/augmented-manifest.html#augmented-manifest-format) in JSON-lines format.\n",
    "\n",
    "Later in the data preparation process we'll have many reasons to slice, dice, and manipulate these manifests via the JSON alone (for example to extract shuffled random subsets for annotation jobs, combine the results of annotation jobs together, etc).\n",
    "\n",
    "For this **initial cataloguing** linking Textract results to page images though, we'll actually **validate that the artifacts are present on S3** in the expected locations.\n",
    "\n",
    "> ⏰ Because of these validation checks, the cell below may a minute or two to run against our 120-document sample set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings = util.preproc.build_data_manifest(\n",
    "    \"data/pages-all-sample.manifest.jsonl\",\n",
    "    rel_doc_paths=rel_filepaths_sample,\n",
    "    textract_s3uri=textract_s3uri,\n",
    "    imgs_s3uri=imgs_s3uri,\n",
    "    by=\"page\",\n",
    "    no_content=\"omit\",\n",
    ")\n",
    "\n",
    "if len(warnings):\n",
    "    raise ValueError(\n",
    "        f\"Manifest incomplete - {len(warnings)} docs failed. Please see `warnings` for details\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's briefly explore the catalogue we've created.\n",
    "\n",
    "Each line of the file is a JSON record identifying a particular page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/pages-all-sample.manifest.jsonl\", \"r\") as f:\n",
    "    for ix, line in enumerate(f):\n",
    "        print(line, end=\"\")\n",
    "        if ix >= 2:\n",
    "            print(\"...\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus has a very skewed distribution of number of pages per document, with a few outliers dragging up the average significantly.\n",
    "\n",
    "In our tests on corpus-wide statistics:\n",
    "\n",
    "- The overall average was **~6.7 pages per document**\n",
    "- The 25th percentile was 3 pages; the 50th percentile was 6 pages; and the 75th percentile was 11 pages\n",
    "- The longest document was 402 pages\n",
    "\n",
    "Your results for sub-sampled sets will likely vary a little - but can be analyzed as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/pages-all-sample.manifest.jsonl\", \"r\") as f:\n",
    "    manifest_df = pd.DataFrame([json.loads(l) for l in f])\n",
    "page_counts_by_doc = manifest_df.groupby(\"textract-ref\")[\"textract-ref\"].count()\n",
    "\n",
    "print(\"Document page count statistics\")\n",
    "page_counts_by_doc.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From visually inspecting some sample documents, we found that the first page was often most useful for the kinds of fields defined for extraction:\n",
    "\n",
    "Many documents used the first page for a fact-sheet/summary, followed by subsequent pages of dense legal terms.\n",
    "\n",
    "Therefore (since our annotation will be image/page-based rather than document-based) we'll aim to include proportionally more first pages when choosing datasets to annotate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation infrastructure\n",
    "\n",
    "To create a labelling job in Amazon SageMaker Ground Truth, we'll need to specify\n",
    "\n",
    "- **Who's** doing the labelling - which could be your own internal teams, the public crowd via Amazon Mechanical Turk, or skilled workers supplied by vendors through the AWS Marketplace\n",
    "- **What** the task will look like - which could be using the [built-in task UIs](https://docs.aws.amazon.com/sagemaker/latest/dg/sms.html) or [custom workflows](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-custom-templates.html).\n",
    "- **Where** the input data sourced from and the results will be saved to (locations on Amazon S3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a private workteam\n",
    "\n",
    "For this demo, you'll set up a private work \"team\" for just yourself to test out the annotation process.\n",
    "\n",
    "▶️ **Open** the [Amazon SageMaker Ground Truth console, *Labeling Workforces* page](https://console.aws.amazon.com/sagemaker/groundtruth?#/labeling-workforces)\n",
    "\n",
    "> ⚠️ **Check** SM Ground Truth opens in the same **AWS Region** where this notebook and your CloudFormation stack are deployed: You may find it defaults to `N. Virginia`. Use the drop-down in the top right of the screen to switch regions.\n",
    "\n",
    "▶️ **Select** the *Private* tab and click **Create private team**\n",
    "\n",
    "- Choose an appropriate **name** for your team e.g. `just-me`\n",
    "- (If you get the option) select to **Invite new workers via email** and enter your email address (you'll need access to this address to log in and annotate the data)\n",
    "- And leave the other (Cognito, SNS, etc) parameters as default.\n",
    "\n",
    "▶️ **If you didn't get the option** to add workers during team creation (typically because your account is already set up for SageMaker Ground Truth), then after the team is created you can:\n",
    "\n",
    "- Click **Invite new workers** to add your email address to the workforce, and then\n",
    "- Click on your **team name** to open the team details, then navigate to the *Workers tab* to add yourself to the team\n",
    "\n",
    "▶️ **Copy** the *name* of your workteam and paste it into the cell below, to store it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workteam_name = \"just-me\"  # TODO: Update this to match yours, if different\n",
    "\n",
    "workteam_arn = util.smgt.workteam_arn_from_name(workteam_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally:\n",
    "\n",
    "▶️ **Check your email** for an invitation and log in to the labelling portal. You'll be asked to configure a password on first login.\n",
    "\n",
    "\n",
    "Your completed setup should look something like this in the AWS Console:\n",
    "\n",
    "![](img/smgt-private-workforce.png \"Screenshot of SageMaker Ground Truth private workforces configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the custom task template\n",
    "\n",
    "This sample provides 2 options for data annotation:\n",
    "\n",
    "1. Use the **built-in [Bounding Box tool](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-bounding-box.html)**\n",
    "2. Use the provided **custom task template** which collects transcription reviews as well as bounding boxes\n",
    "\n",
    "We recommend **at least experimenting with the custom template** here in the notebook, to get a better understanding of how the model will \"see\" and use your annotations (and how you might extend this sample for your own use cases).\n",
    "\n",
    "However, you'll probably want to use the built-in boxes tool for the bulk of your annotating work because:\n",
    "\n",
    "- The ML model we present (in the next notebook) only supports tagging and cannot be directly trained on the text corrections you collect in the custom template\n",
    "- ...And reviewing the text transcription takes extra time & effort\n",
    "\n",
    "As detailed [in the developer guide](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-custom-templates.html), custom Ground Truth UIs are HTML [Liquid templates](https://shopify.github.io/liquid/basics/introduction/). You can use the [Crowd HTML Elements](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-ui-template-reference.html) to embed standard components, but also include custom HTML/CSS/JS as needed. A set of examples is provided in the [amazon-sagemaker-ground-truth-task-uis repository on GitHub](https://github.com/aws-samples/amazon-sagemaker-ground-truth-task-uis).\n",
    "\n",
    "Since spinning up a labelling job each time to test and debug a custom template would slow down development, SageMaker provides a [RenderUiTemplate API](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_RenderUiTemplate.html) for previewing the worker experience.\n",
    "\n",
    "First, we'll populate the master template `*.liquid.tpl.html` with the entity/field types we configured earlier (and some other automated content) to produce the final SageMaker Ground Truth template `*.liquid.html`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"annotation/ocr-bbox-and-validation.liquid.tpl.html\", \"r\") as ftpl:\n",
    "    with open(\"annotation/ocr-bbox-and-validation.liquid.html\", \"w\") as fout:\n",
    "        template = BeautifulSoup(ftpl.read())\n",
    "\n",
    "        annotator_el = template.find(id=\"annotator\")\n",
    "        annotator_el[\"header\"] = \"Highlight entities and review their OCR results.\"\n",
    "        annotator_el[\"labels\"] = json.dumps(entity_classes)\n",
    "\n",
    "        if any(f.annotation_guidance for f in fields):\n",
    "            full_instructions_el = template.find(\"full-instructions\")\n",
    "            full_instructions_el.append(\n",
    "                BeautifulSoup(\n",
    "                    \"\\n\".join(\n",
    "                        [\"<h3>Per-Field Guidance</h3>\"]\n",
    "                        + [\n",
    "                            f\"<h4>{f.name}</h4>\\n{f.annotation_guidance}\"\n",
    "                            for f in fields if f.annotation_guidance\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        fout.write(template.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to serve our example images through the UI, SageMaker Ground Truth requires the target S3 bucket to be set up [with CORS permissions](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-cors-update.html) (which is not the same as making the bucket or its contents public).\n",
    "\n",
    "The cell below will ensure these permissions are set on the bucket configured earlier by `bucket_name`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\"s3\")\n",
    "bucket_cors = s3.BucketCors(bucket_name)\n",
    "\n",
    "try:\n",
    "    existing_rules = bucket_cors.cors_rules\n",
    "except:\n",
    "    existing_rules = []\n",
    "\n",
    "if any(\n",
    "    r for r in existing_rules\n",
    "    if \"*\" in r[\"AllowedOrigins\"]\n",
    "    and \"GET\" in r[\"AllowedMethods\"]\n",
    "):\n",
    "    logger.info(f\"Bucket already set up with CORS permissions\")\n",
    "else:\n",
    "    new_rules = existing_rules + [\n",
    "        {\n",
    "            \"ID\": \"SageMakerGroundTruth\",\n",
    "            \"AllowedHeaders\": [],\n",
    "            \"AllowedMethods\": [\"GET\"],\n",
    "            \"AllowedOrigins\": [\"*\"],\n",
    "            \"ExposeHeaders\": [],\n",
    "            'MaxAgeSeconds': 60,\n",
    "        },\n",
    "    ]\n",
    "    cors_resp = bucket_cors.put(\n",
    "        CORSConfiguration={ \"CORSRules\": new_rules },\n",
    "        ExpectedBucketOwner=os.environ[\"AWS_ACCOUNT_ID\"],\n",
    "    )\n",
    "    logger.info(f\"Added CORS permissions to bucket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to render the preview of what this task would look like with an actual record from the data manifest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "smclient = boto3.client(\"sagemaker\")\n",
    "\n",
    "# Fetch an example record from the manifest:\n",
    "ix_example = 0\n",
    "with open(\"data/pages-all-sample.manifest.jsonl\", \"r\") as fmanifest:\n",
    "    sample_task_str = None\n",
    "    for ix, line in enumerate(fmanifest):\n",
    "        if ix == ix_example:\n",
    "            sample_task_str = line\n",
    "            break\n",
    "\n",
    "# Render the template with the example record:\n",
    "ui_render_file = \"annotation/render.tmp.html\"\n",
    "with open(\"annotation/ocr-bbox-and-validation.liquid.html\", \"r\") as fui:\n",
    "    with open(ui_render_file, \"w\") as frender:\n",
    "        ui_render_resp = smclient.render_ui_template(\n",
    "            UiTemplate={ \"Content\": fui.read() },\n",
    "            Task={ \"Input\": sample_task_str },\n",
    "            RoleArn=role,\n",
    "        )\n",
    "        frender.write(ui_render_resp[\"RenderedContent\"])\n",
    "\n",
    "print(f\"▶️ Open {ui_render_file} and click 'Trust HTML' to see the UI in action!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening [annotation/render.tmp.html](annotation/render.tmp.html) and clicking **Trust HTML** in the toolbar, you should see a view something similar to the below:\n",
    "\n",
    "> ℹ️ **Note:** In this task template, you need to click the \"Instructions\" button to expand the transcription review pane on the left!\n",
    "\n",
    "![](img/smgt-custom-template-demo.png \"Screenshot of custom annotation UI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that:\n",
    "\n",
    "- When you draw a bounding box on the page image, a new OCR result is populated in the left sidebar prompting you to review (and if necessary correct) Textract's transcription of the text in that region.\n",
    "- Overlapping bounding boxes of the same type are consolidated, allowing us to highlight non-square regions of text (for example a particular sentence over multiple lines within a paragraph).\n",
    "- Transcription review fields are mandatory: The template should not let you submit the result until all transcriptions have been reviewed.\n",
    "\n",
    "You should aim to follow these same conventions when annotating the sample data, even with the built-in task type. Under the hood, the ML model code applies similar logic to map your bounding box annotations to the Textract detected `WORD`s and `LINE`s.\n",
    "\n",
    "To use this custom template in a data labeling job, you can adjust the instructions below (which assume you'll use the faster built-in template) as follows:\n",
    "\n",
    "- Select task category 'Custom' > task type 'Custom', instead of 'Image > Bounding Box'\n",
    "- For template body, copy the contents of the `*.liquid.html` file above (**NOT** the `*.tpl.liquid.html`, which has placeholders e.g. for the list of classes)\n",
    "- In the tool configuration step, select the `SMGT-Pre` and `SMGT-Post` Lambda functions that have been created for you by the solution stack: These should appear in the drop-down options.\n",
    "\n",
    "In practice, while it's important to explore how the bounding boxes are being interpreted, we'd recommend to use the simpler built-in template for this walkthrough: To help you complete your data annotation faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate data\n",
    "\n",
    "> ⏰ **If you're short on time**: You can skip the remaining steps in this notebook altogether.\n",
    ">\n",
    "> We've provided pre-prepared annotations for 100 pages in the `data/annotations` folder, to augment your work and help train an effective model faster. If you need, you can skip along to the next notebook and select **only** the `augmentation-*` datasets instead of labeling your own too. If you choose to do this, your model will likely be less accurate.\n",
    "\n",
    "We're now ready to start annotating data, and will typically **iterate over multiple jobs** in this step to start small and then boost model accuracy.\n",
    "\n",
    "To make incrementally adding to the dataset easy, we'll need to pay particular attention to:\n",
    "\n",
    "- How we sample data for jobs, with good randomness but no repetition of previously-annotated pages\n",
    "- How we collect our results to a single consolidated dataset\n",
    "\n",
    "So let's follow through the steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect a dataset\n",
    "\n",
    "Here we will:\n",
    "\n",
    "- **Shuffle** our data (in a *reproducible*/deterministic way), to ensure we annotate documents/pages from a range of providers - not just concentrating on the first provider/doc(s)\n",
    "- **Exclude** any examples for which the page image has **already been labeled** in the `data/annotations` output folder\n",
    "- **Stratify** the sample, to obtain a specific (boosted) proportion of first-page samples, since we observed the first pages of documents to often be most useful for the fields of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell just defines the necessary functions & constants:\n",
    "\n",
    "# Keep this the same across the jobs:\n",
    "annotations_base_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/annotations\"\n",
    "\n",
    "def rel_path_from_s3uri(uri, key_base=\"data/imgs-clean/\") -> str:\n",
    "    \"\"\"Extract e.g. 'subfolders/file' from 's3://bucket/.../{key_base}subfolders/file'\"\"\"\n",
    "    return uri[len(\"s3://\"):].partition(\"/\")[2].partition(key_base)[2]\n",
    "\n",
    "\n",
    "def get_preannotated_imgs(exclude_job_names=[]) -> set:\n",
    "    \"\"\"Find the set of relative image paths that have already been annotated\"\"\"\n",
    "    filepaths = set()  # Protect against introducing duplicates\n",
    "    for job_folder in os.listdir(\"data/annotations\"):\n",
    "        if job_folder in exclude_job_names:\n",
    "            logger.info(f\"Skipping excluded job {job_folder}\")\n",
    "            continue\n",
    "        manifest_file = os.path.join(\n",
    "            \"data\",\n",
    "            \"annotations\",\n",
    "            job_folder,\n",
    "            \"manifests\",\n",
    "            \"output\",\n",
    "            \"output.manifest\",\n",
    "        )\n",
    "        if not os.path.isfile(manifest_file):\n",
    "            if os.path.isdir(os.path.join(\"data\", \"annotations\", job_folder)):\n",
    "                logger.warning(f\"Skipping job {job_folder}: No output manifest at {manifest_file}\")\n",
    "            continue\n",
    "        with open(manifest_file, \"r\") as f:\n",
    "            filepaths.update([\n",
    "                rel_path_from_s3uri(json.loads(l)[\"source-ref\"])\n",
    "                for l in f\n",
    "            ])\n",
    "    return filepaths\n",
    "\n",
    "\n",
    "def select_examples(\n",
    "    job_page_count,\n",
    "    exclude_img_paths=set(),\n",
    "    job_first_page_pct=0.4,\n",
    "):\n",
    "\n",
    "    with open(\"data/pages-all-sample.manifest.jsonl\", \"r\") as fmanifest:\n",
    "        examples_all = [json.loads(l) for l in fmanifest]\n",
    "\n",
    "    # Separate and shuffle the first vs non-first pages:\n",
    "    examples_all_arefirsts = [l[\"page-num\"] == 1 for l in examples_all]\n",
    "\n",
    "    examples_firsts = [e for ix, e in enumerate(examples_all) if examples_all_arefirsts[ix]]\n",
    "    examples_nonfirsts = [e for ix, e in enumerate(examples_all) if not examples_all_arefirsts[ix]]\n",
    "    random.Random(1337).shuffle(examples_firsts)\n",
    "    random.Random(1337).shuffle(examples_nonfirsts)\n",
    "\n",
    "    # Exclude already-annotated images:\n",
    "    filtered_firsts = [\n",
    "        e for e in examples_firsts\n",
    "        if rel_path_from_s3uri(e[\"source-ref\"]) not in exclude_img_paths\n",
    "    ]\n",
    "    filtered_nonfirsts = [\n",
    "        e for e in examples_nonfirsts\n",
    "        if rel_path_from_s3uri(e[\"source-ref\"]) not in exclude_img_paths\n",
    "    ]\n",
    "    print(f\"Excluded {len(examples_firsts) - len(filtered_firsts)} first and {len(examples_nonfirsts) - len(filtered_nonfirsts)} non-first pages\")\n",
    "    \n",
    "    # Draw from the filtered shuffled lists:\n",
    "    n_first_pages = round(job_first_page_pct * job_page_count)\n",
    "    n_nonfirst_pages = job_page_count - n_first_pages\n",
    "    if n_first_pages > len(filtered_firsts):\n",
    "        raise ValueError(\n",
    "            \"Unable to find enough first-page records to build manifest: Wanted \"\n",
    "            \"{}, but only {} available from list after exclusions ({} before)\".format(\n",
    "                n_first_pages,\n",
    "                len(filtered_firsts),\n",
    "                len(examples_firsts),\n",
    "            )\n",
    "        )\n",
    "    if n_nonfirst_pages > len(filtered_nonfirsts):\n",
    "        raise ValueError(\n",
    "            \"Unable to find enough non-first-page records to build manifest: Wanted \"\n",
    "            \"{}, but only {} available from list after exclusions ({} before)\".format(\n",
    "                n_nonfirst_pages,\n",
    "                len(filtered_nonfirsts),\n",
    "                len(examples_nonfirsts),\n",
    "            )\n",
    "        )\n",
    "    print(f\"Taking {n_first_pages} first pages and {n_nonfirst_pages} non-first pages.\")\n",
    "    selected = filtered_firsts[:n_first_pages] + filtered_nonfirsts[:n_nonfirst_pages]\n",
    "    random.Random(1337).shuffle(selected)  # Shuffle again to avoid putting all 1stP at front\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually generate a new job input manifest, you just need to specify:\n",
    "\n",
    "- A unique name for the job\n",
    "- The number of examples (pages) you'll annotate\n",
    "- The ratio of first-pages to non-first pages (e.g. 0.4 -> 40% of examples will be the first page of a document)\n",
    "\n",
    "> ⚠️ **Warning:** If you've just completed an annotation job below, make sure you've `s3 sync`ed results back to the `data/annotations` folder - otherwise you'll set up a new job for the same pages again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_job_name = \"cfpb-boxes-1\"  # What will this job be called?\n",
    "job_page_count = 20  # How many pages will we annotate?\n",
    "job_first_page_pct = .4  # What proportion of pages should be first pages of a doc?\n",
    "\n",
    "\n",
    "preannotated_img_paths = get_preannotated_imgs()\n",
    "input_manifest_file = f\"data/manifests/{annotation_job_name}.jsonl\"\n",
    "os.makedirs(\"data/manifests\", exist_ok=True)\n",
    "print(f\"'{annotation_job_name}' saving to: {input_manifest_file}\")\n",
    "with open(input_manifest_file, \"w\") as f:\n",
    "    for ix, example in enumerate(select_examples(\n",
    "        job_page_count,\n",
    "        exclude_img_paths=preannotated_img_paths,\n",
    "        job_first_page_pct=job_first_page_pct,\n",
    "    )):\n",
    "        if ix < 3:\n",
    "            print(example)\n",
    "        elif ix == 3:\n",
    "            print(\"...\")\n",
    "        f.write(json.dumps(example) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_manifest_s3uri = f\"s3://{bucket_name}/{bucket_prefix}{input_manifest_file}\"\n",
    "!aws s3 cp $input_manifest_file $input_manifest_s3uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the labelling job\n",
    "\n",
    "To minimize the risk of errors and get started quickly, you're recommended to create your labeling job by running the utility function provided below.\n",
    "\n",
    "This will set up a job with the default pre-built bounding box template (for faster annotation than the custom one we explored earlier):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting labeling job {annotation_job_name}\\non data {input_manifest_s3uri}\\n\")\n",
    "create_labeling_job_resp = util.smgt.create_bbox_labeling_job(\n",
    "    annotation_job_name,\n",
    "    bucket_name=bucket_name,\n",
    "    execution_role_arn=role,\n",
    "    fields=fields,\n",
    "    input_manifest_s3uri=input_manifest_s3uri,\n",
    "    output_s3uri=annotations_base_s3uri,\n",
    "    workteam_arn=workteam_arn,\n",
    "    # To create a review/adjustment job from a manifest with existing labels in:\n",
    "    #reviewing_attribute_name=\"label\",\n",
    "    s3_inputs_prefix=f\"{bucket_prefix}data/manifests\",\n",
    ")\n",
    "print(f\"\\nLABELLING JOB STARTED:\\n{create_labeling_job_resp['LabelingJobArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can also explore creating the job through the [AWS Console for SageMaker Ground Truth](https://console.aws.amazon.com/sagemaker/groundtruth?#/labeling-jobs) (check your AWS Region!) by clicking on *Create labeling job*:\n",
    "\n",
    "- Leave (as default) the *label attribute name* the same as the *job name*\n",
    "- Select **Manual data setup** and use:\n",
    "  - The `input_manifest_s3uri` (`s3://[...].jsonl`) from above for the input location\n",
    "  - The `annotations_base_s3uri` (`s3://[...]/data/annotations`) with **no trailing slash** for the output location\n",
    "- Select or create any **SageMaker IAM execution role** that has access to the `bucket_name` we're using.\n",
    "- For **task type**, select *Image > Bounding Box*\n",
    "- On the second screen, be sure to use **worker type** *Private* and select the workteam we made earlier from the dropdown.\n",
    "- For the built-in task type, you'll need to enter the **labels** manually exactly in the order that we defined them in this notebook.\n",
    "\n",
    "The cell below prints out some of these values to help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_manifest_s3uri)\n",
    "print(annotations_base_s3uri)\n",
    "print(role)\n",
    "print(\"\\n\".join([\"\\nLabels:\", \"-------\"] + entity_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label the data!\n",
    "\n",
    "Now that the labeling job has been created, you'll see a new task for your user in the labeling portal (If you lost the portal link from your email, you can access it from the *Private* tab of the [SageMaker Ground Truth Workforces console](https://console.aws.amazon.com/sagemaker/groundtruth?#/labeling-workforces)).\n",
    "\n",
    "> ⏰ SageMaker Ground Truth processes the job data in batches, so it might take a minute or two for the job to appear in your list.\n",
    ">\n",
    "> If it's taking a long time, you can:\n",
    ">\n",
    "> - Double-check the job in the [Labeling jobs page of the Console](https://console.aws.amazon.com/sagemaker/groundtruth?#/labeling-jobs) to see if it's failed to start due to some error\n",
    "> - Check the job is set up for a workteam that you're a member of\n",
    "> - Check your user is showing as *Verified* and *Enabled* (i.e. that you completed the email verification successfully) in the *Private* tab of the [Workforces console](https://console.aws.amazon.com/sagemaker/groundtruth?#/labeling-workforces)\n",
    "\n",
    "▶️ Click **Start working** and annotate the examples until the all are finished and you're returned to the portal homepage.\n",
    "\n",
    "▶️ **Try to be as consistent as possible** in how you annotate the classes, because inconsistent annotations can significantly degrade final model accuracy. Refer to the guidance (in this notebook and the 'Full Instructions') that we applied when annotating the example set.\n",
    "\n",
    "![](img/smgt-task-pending.png \"Screenshot of SMGT labeling portal with pending task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sync the results locally (and iterate?)\n",
    "\n",
    "Once you've finished annotating and the job shows as \"Complete\" in the [SMGT Console](https://console.aws.amazon.com/sagemaker/groundtruth?#/labeling-jobs) (which **might take an extra minute or two**, while your annotations are consolidated), you can download the results here to the notebook via the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync --quiet $annotations_base_s3uri ./data/annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a subfolder created with the name of your annotation job, under which the **`manifests/output/output.manifest`** file contains the consolidated results of your labelling - again in the open JSON-Lines format.\n",
    "\n",
    "▶️ **Check** your results appear as expected, and explore the file format.\n",
    "\n",
    "> Because label outputs are in JSON-Lines, it's easy to consolidate, transform, and manipulate these results as required using open source tools!\n",
    "\n",
    "If you like, you can expand your dataset with **additional labelling jobs** by repeating these steps from [Collect a dataset](#Collect-a-dataset) down to here.\n",
    "\n",
    "> ⚠️ Take care to set a different `annotation_job_name` each time, as these must be unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In this notebook we set up the modelling objective, collected the project dataset, and annotated (perhaps multiple) sets of training data.\n",
    "\n",
    "In the next, we'll consolidate these output manifests (together with some pre-prepared example data) and actually train/deploy our ML model.\n",
    "\n",
    "So you can now open up **notebook [2. Model Training.ipynb](2.%20Model%20Training.ipynb)**, and follow along!"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-1:492261229750:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
